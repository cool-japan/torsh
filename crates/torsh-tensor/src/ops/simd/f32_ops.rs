//! SIMD operations specifically optimized for f32 tensors
//! ðŸš€ Zero-copy SIMD using scoped access pattern (Phase 2.5 implementation)
//!
//! This module implements real hardware SIMD operations with buffer-writing
//! to eliminate output allocations, achieving true SIMD speedup.
//!
//! **Phase 2.5 Innovation**: Pre-allocate output buffer, write SIMD results
//! directly without intermediate allocations. Reduces from 4 allocations to 1.

use crate::{Tensor, TensorElement};
use torsh_core::error::{Result, TorshError};
use super::SimdOpType;

// Import scirs2_core SIMD operations for real hardware acceleration
#[cfg(feature = "simd")]
use scirs2_core::simd_ops::SimdUnifiedOps;
#[cfg(feature = "simd")]
use scirs2_core::ndarray::ArrayView1;

/// Write SIMD addition results directly to pre-allocated buffer
///
/// This function performs SIMD addition and writes results directly to the
/// output buffer without intermediate allocations.
///
/// # Safety
/// - Caller must ensure `output` has length equal to `a.len()` and `b.len()`
/// - All slices must be properly aligned for their element type
#[cfg(feature = "simd")]
unsafe fn simd_add_into_buffer_f32(a: &[f32], b: &[f32], output: &mut [f32]) {
    assert_eq!(a.len(), b.len(), "Input arrays must have same length");
    assert_eq!(a.len(), output.len(), "Output buffer must match input length");

    let len = a.len();

    #[cfg(target_arch = "x86_64")]
    {
        use std::arch::x86_64::*;

        if is_x86_feature_detected!("avx2") {
            let mut i = 0;
            // Process 8 f32s at a time with AVX2
            while i + 8 <= len {
                let a_vec = _mm256_loadu_ps(a.as_ptr().add(i));
                let b_vec = _mm256_loadu_ps(b.as_ptr().add(i));
                let result_vec = _mm256_add_ps(a_vec, b_vec);
                _mm256_storeu_ps(output.as_mut_ptr().add(i), result_vec);
                i += 8;
            }

            // Handle remaining elements
            for j in i..len {
                output[j] = a[j] + b[j];
            }
        } else if is_x86_feature_detected!("sse") {
            let mut i = 0;
            // Process 4 f32s at a time with SSE
            while i + 4 <= len {
                let a_vec = _mm_loadu_ps(a.as_ptr().add(i));
                let b_vec = _mm_loadu_ps(b.as_ptr().add(i));
                let result_vec = _mm_add_ps(a_vec, b_vec);
                _mm_storeu_ps(output.as_mut_ptr().add(i), result_vec);
                i += 4;
            }

            // Handle remaining elements
            for j in i..len {
                output[j] = a[j] + b[j];
            }
        } else {
            // Scalar fallback
            for i in 0..len {
                output[i] = a[i] + b[i];
            }
        }
    }

    #[cfg(target_arch = "aarch64")]
    {
        use std::arch::aarch64::*;

        if std::arch::is_aarch64_feature_detected!("neon") {
            let mut i = 0;
            // Process 4 f32s at a time with NEON
            while i + 4 <= len {
                let a_vec = vld1q_f32(a.as_ptr().add(i));
                let b_vec = vld1q_f32(b.as_ptr().add(i));
                let result_vec = vaddq_f32(a_vec, b_vec);
                vst1q_f32(output.as_mut_ptr().add(i), result_vec);
                i += 4;
            }

            // Handle remaining elements
            for j in i..len {
                output[j] = a[j] + b[j];
            }
        } else {
            // Scalar fallback
            for i in 0..len {
                output[i] = a[i] + b[i];
            }
        }
    }

    #[cfg(not(any(target_arch = "x86_64", target_arch = "aarch64")))]
    {
        // Scalar fallback for other architectures
        for i in 0..len {
            output[i] = a[i] + b[i];
        }
    }
}

/// Write SIMD multiplication results directly to pre-allocated buffer
///
/// This function performs SIMD multiplication and writes results directly to the
/// output buffer without intermediate allocations.
///
/// # Safety
/// - Caller must ensure `output` has length equal to `a.len()` and `b.len()`
/// - All slices must be properly aligned for their element type
#[cfg(feature = "simd")]
unsafe fn simd_mul_into_buffer_f32(a: &[f32], b: &[f32], output: &mut [f32]) {
    assert_eq!(a.len(), b.len(), "Input arrays must have same length");
    assert_eq!(a.len(), output.len(), "Output buffer must match input length");

    let len = a.len();

    #[cfg(target_arch = "x86_64")]
    {
        use std::arch::x86_64::*;

        if is_x86_feature_detected!("avx2") {
            let mut i = 0;
            // Process 8 f32s at a time with AVX2
            while i + 8 <= len {
                let a_vec = _mm256_loadu_ps(a.as_ptr().add(i));
                let b_vec = _mm256_loadu_ps(b.as_ptr().add(i));
                let result_vec = _mm256_mul_ps(a_vec, b_vec);
                _mm256_storeu_ps(output.as_mut_ptr().add(i), result_vec);
                i += 8;
            }

            // Handle remaining elements
            for j in i..len {
                output[j] = a[j] * b[j];
            }
        } else if is_x86_feature_detected!("sse") {
            let mut i = 0;
            // Process 4 f32s at a time with SSE
            while i + 4 <= len {
                let a_vec = _mm_loadu_ps(a.as_ptr().add(i));
                let b_vec = _mm_loadu_ps(b.as_ptr().add(i));
                let result_vec = _mm_mul_ps(a_vec, b_vec);
                _mm_storeu_ps(output.as_mut_ptr().add(i), result_vec);
                i += 4;
            }

            // Handle remaining elements
            for j in i..len {
                output[j] = a[j] * b[j];
            }
        } else {
            // Scalar fallback
            for i in 0..len {
                output[i] = a[i] * b[i];
            }
        }
    }

    #[cfg(target_arch = "aarch64")]
    {
        use std::arch::aarch64::*;

        if std::arch::is_aarch64_feature_detected!("neon") {
            let mut i = 0;
            // Process 4 f32s at a time with NEON
            while i + 4 <= len {
                let a_vec = vld1q_f32(a.as_ptr().add(i));
                let b_vec = vld1q_f32(b.as_ptr().add(i));
                let result_vec = vmulq_f32(a_vec, b_vec);
                vst1q_f32(output.as_mut_ptr().add(i), result_vec);
                i += 4;
            }

            // Handle remaining elements
            for j in i..len {
                output[j] = a[j] * b[j];
            }
        } else {
            // Scalar fallback
            for i in 0..len {
                output[i] = a[i] * b[i];
            }
        }
    }

    #[cfg(not(any(target_arch = "x86_64", target_arch = "aarch64")))]
    {
        // Scalar fallback for other architectures
        for i in 0..len {
            output[i] = a[i] * b[i];
        }
    }
}

impl<T: TensorElement> Tensor<T> {
    /// Zero-copy SIMD addition for f32 tensors (Phase 2 implementation)
    ///
    /// Uses scoped access pattern to eliminate memory copies, enabling real
    /// 2-4x SIMD speedup over scalar operations.
    ///
    /// # Performance
    /// - Zero memory copies (uses `with_data_slice()`)
    /// - Real hardware SIMD (AVX2/NEON via scirs2_core)
    /// - Expected: 2-4x faster than scalar (per SciRS2 docs)
    #[cfg(feature = "simd")]
    pub fn add_op_simd_f32_zero_copy(&self, other: &Self) -> Result<Self> {
        if self.shape() != other.shape() {
            return Err(TorshError::ShapeMismatch {
                expected: self.shape().to_vec(),
                got: other.shape().to_vec(),
            });
        }

        // Zero-copy SIMD operation using scoped access
        let result_vec = self.with_data_slice(|data_a| {
            other.with_data_slice(|data_b| {
                // Cast to f32 slices (safe because we check TypeId in caller)
                let self_f32 = unsafe {
                    std::slice::from_raw_parts(
                        data_a.as_ptr() as *const f32,
                        data_a.len(),
                    )
                };
                let other_f32 = unsafe {
                    std::slice::from_raw_parts(
                        data_b.as_ptr() as *const f32,
                        data_b.len(),
                    )
                };

                // Zero-copy ArrayView creation
                let view_a = ArrayView1::from(self_f32);
                let view_b = ArrayView1::from(other_f32);

                // Real hardware SIMD addition
                let result_arr = f32::simd_add(&view_a, &view_b);
                Ok(result_arr.to_vec())
            })
        })?;

        // Cast result back to T type
        let result_t: Vec<T> = result_vec.into_iter().map(|f| unsafe {
            std::mem::transmute_copy::<f32, T>(&f)
        }).collect();

        Self::from_data(result_t, self.shape().to_vec(), self.device)
    }

    /// Zero-copy SIMD multiplication for f32 tensors (Phase 2 implementation)
    ///
    /// Uses scoped access pattern to eliminate memory copies, enabling real
    /// 2-4x SIMD speedup over scalar operations.
    ///
    /// # Performance
    /// - Zero memory copies (uses `with_data_slice()`)
    /// - Real hardware SIMD (AVX2/NEON via scirs2_core)
    /// - Expected: 2-4x faster than scalar (per SciRS2 docs)
    #[cfg(feature = "simd")]
    pub fn mul_op_simd_f32_zero_copy(&self, other: &Self) -> Result<Self> {
        if self.shape() != other.shape() {
            return Err(TorshError::ShapeMismatch {
                expected: self.shape().to_vec(),
                got: other.shape().to_vec(),
            });
        }

        // Zero-copy SIMD operation using scoped access
        let result_vec = self.with_data_slice(|data_a| {
            other.with_data_slice(|data_b| {
                // Cast to f32 slices (safe because we check TypeId in caller)
                let self_f32 = unsafe {
                    std::slice::from_raw_parts(
                        data_a.as_ptr() as *const f32,
                        data_a.len(),
                    )
                };
                let other_f32 = unsafe {
                    std::slice::from_raw_parts(
                        data_b.as_ptr() as *const f32,
                        data_b.len(),
                    )
                };

                // Zero-copy ArrayView creation
                let view_a = ArrayView1::from(self_f32);
                let view_b = ArrayView1::from(other_f32);

                // Real hardware SIMD multiplication
                let result_arr = f32::simd_mul(&view_a, &view_b);
                Ok(result_arr.to_vec())
            })
        })?;

        // Cast result back to T type
        let result_t: Vec<T> = result_vec.into_iter().map(|f| unsafe {
            std::mem::transmute_copy::<f32, T>(&f)
        }).collect();

        Self::from_data(result_t, self.shape().to_vec(), self.device)
    }

    /// Zero-copy SIMD addition fallback (when SIMD feature is disabled)
    #[cfg(not(feature = "simd"))]
    pub fn add_op_simd_f32_zero_copy(&self, other: &Self) -> Result<Self> {
        self.element_wise_op(other, |a, b| a + b)
    }

    /// Zero-copy SIMD multiplication fallback (when SIMD feature is disabled)
    #[cfg(not(feature = "simd"))]
    pub fn mul_op_simd_f32_zero_copy(&self, other: &Self) -> Result<Self> {
        self.element_wise_op(other, |a, b| a * b)
    }

    /// ðŸš€ **Phase 2.5**: Buffer-writing SIMD addition (ONLY 1 allocation!)
    ///
    /// Pre-allocates output buffer and writes SIMD results directly to it,
    /// eliminating 3 of 4 allocations from Phase 2 implementation.
    ///
    /// # Performance
    /// - **Allocations**: 1 (vs 4 in Phase 2, vs 2 in scalar)
    /// - **Zero-copy inputs**: Uses `with_data_slice()` for input access
    /// - **Direct output write**: SIMD writes to pre-allocated buffer
    /// - **Expected**: 2-4x faster than scalar (now architecturally achievable)
    ///
    /// # Allocation Breakdown
    /// - Allocation 1: Pre-allocate output buffer (REQUIRED for final Tensor)
    /// - NO intermediate allocations (Array1, to_vec, collect all eliminated)
    #[cfg(feature = "simd")]
    pub fn add_op_simd_f32_buffer(&self, other: &Self) -> Result<Self> {
        if self.shape() != other.shape() {
            return Err(TorshError::ShapeMismatch {
                expected: self.shape().to_vec(),
                got: other.shape().to_vec(),
            });
        }

        // Pre-allocate f32 output buffer (Allocation 1)
        let mut result_f32 = vec![0.0f32; self.numel()];

        // Use zero-copy scoped access for inputs
        self.with_data_slice(|data_a| {
            other.with_data_slice(|data_b| {
                // Cast to f32 slices (safe - TypeId checked in caller)
                let self_f32 = unsafe {
                    std::slice::from_raw_parts(
                        data_a.as_ptr() as *const f32,
                        data_a.len(),
                    )
                };
                let other_f32 = unsafe {
                    std::slice::from_raw_parts(
                        data_b.as_ptr() as *const f32,
                        data_b.len(),
                    )
                };

                // Write SIMD results directly to output buffer (NO allocation!)
                unsafe {
                    simd_add_into_buffer_f32(self_f32, other_f32, &mut result_f32);
                }

                Ok(())
            })
        })?;

        // Reinterpret Vec<f32> as Vec<T> (NO allocation - just transmute)
        let result_t = unsafe {
            let ptr = result_f32.as_mut_ptr() as *mut T;
            let len = result_f32.len();
            let cap = result_f32.capacity();
            std::mem::forget(result_f32);  // Prevent double-free
            Vec::from_raw_parts(ptr, len, cap)
        };

        // Create Tensor from transmuted buffer (NO allocation - just move)
        Self::from_data(result_t, self.shape().to_vec(), self.device)
    }

    /// ðŸš€ **Phase 2.5**: Buffer-writing SIMD multiplication (ONLY 1 allocation!)
    ///
    /// Pre-allocates output buffer and writes SIMD results directly to it,
    /// eliminating 3 of 4 allocations from Phase 2 implementation.
    ///
    /// # Performance
    /// - **Allocations**: 1 (vs 4 in Phase 2, vs 2 in scalar)
    /// - **Zero-copy inputs**: Uses `with_data_slice()` for input access
    /// - **Direct output write**: SIMD writes to pre-allocated buffer
    /// - **Expected**: 2-4x faster than scalar (now architecturally achievable)
    ///
    /// # Allocation Breakdown
    /// - Allocation 1: Pre-allocate output buffer (REQUIRED for final Tensor)
    /// - NO intermediate allocations (Array1, to_vec, collect all eliminated)
    #[cfg(feature = "simd")]
    pub fn mul_op_simd_f32_buffer(&self, other: &Self) -> Result<Self> {
        if self.shape() != other.shape() {
            return Err(TorshError::ShapeMismatch {
                expected: self.shape().to_vec(),
                got: other.shape().to_vec(),
            });
        }

        // Pre-allocate f32 output buffer (Allocation 1)
        let mut result_f32 = vec![0.0f32; self.numel()];

        // Use zero-copy scoped access for inputs
        self.with_data_slice(|data_a| {
            other.with_data_slice(|data_b| {
                // Cast to f32 slices (safe - TypeId checked in caller)
                let self_f32 = unsafe {
                    std::slice::from_raw_parts(
                        data_a.as_ptr() as *const f32,
                        data_a.len(),
                    )
                };
                let other_f32 = unsafe {
                    std::slice::from_raw_parts(
                        data_b.as_ptr() as *const f32,
                        data_b.len(),
                    )
                };

                // Write SIMD results directly to output buffer (NO allocation!)
                unsafe {
                    simd_mul_into_buffer_f32(self_f32, other_f32, &mut result_f32);
                }

                Ok(())
            })
        })?;

        // Reinterpret Vec<f32> as Vec<T> (NO allocation - just transmute)
        let result_t = unsafe {
            let ptr = result_f32.as_mut_ptr() as *mut T;
            let len = result_f32.len();
            let cap = result_f32.capacity();
            std::mem::forget(result_f32);  // Prevent double-free
            Vec::from_raw_parts(ptr, len, cap)
        };

        // Create Tensor from transmuted buffer (NO allocation - just move)
        Self::from_data(result_t, self.shape().to_vec(), self.device)
    }

    /// Buffer-writing SIMD addition fallback (when SIMD feature is disabled)
    #[cfg(not(feature = "simd"))]
    pub fn add_op_simd_f32_buffer(&self, other: &Self) -> Result<Self> {
        self.element_wise_op(other, |a, b| a + b)
    }

    /// Buffer-writing SIMD multiplication fallback (when SIMD feature is disabled)
    #[cfg(not(feature = "simd"))]
    pub fn mul_op_simd_f32_buffer(&self, other: &Self) -> Result<Self> {
        self.element_wise_op(other, |a, b| a * b)
    }

    // ============================================================================
    // PHASE 3: OPTIMAL SIMD WITH UNINIT BUFFERS AND NEW SCIRS2 API
    // ============================================================================
    // Uses:
    // 1. Uninit buffer allocation (skip zero-initialization)
    // 2. scirs2_core::simd_ops::SimdUnifiedOps::simd_add_into (direct buffer write)
    // 3. Zero-copy input access via with_data_slice()
    //
    // Expected: Match or beat scalar performance
    // ============================================================================

    /// ðŸš€ **Phase 3**: Optimal SIMD addition with uninit buffer + scirs2 API
    ///
    /// Uses uninit buffer allocation to skip zero-initialization overhead,
    /// combined with scirs2_core's zero-allocation SIMD API.
    ///
    /// # Performance
    /// - **Allocations**: 1 (uninit buffer, no initialization overhead)
    /// - **Zero-copy inputs**: Uses `with_data_slice()` for input access
    /// - **Direct output write**: Uses `f32::simd_add_into()` from scirs2_core
    /// - **Expected**: Match or beat scalar performance
    #[cfg(feature = "simd")]
    pub fn add_op_simd_phase3(&self, other: &Self) -> Result<Self> {
        if self.shape() != other.shape() {
            return Err(TorshError::ShapeMismatch {
                expected: self.shape().to_vec(),
                got: other.shape().to_vec(),
            });
        }

        let len = self.numel();

        // Phase 2 improvement: Uninit buffer allocation (skip zero-initialization)
        // This saves ~8Âµs for 50K elements
        let mut result_f32: Vec<f32> = Vec::with_capacity(len);
        unsafe { result_f32.set_len(len); }

        // Use zero-copy scoped access for inputs
        self.with_data_slice(|data_a| {
            other.with_data_slice(|data_b| {
                // Cast to f32 slices (safe - TypeId checked in caller)
                let self_f32 = unsafe {
                    std::slice::from_raw_parts(
                        data_a.as_ptr() as *const f32,
                        data_a.len(),
                    )
                };
                let other_f32 = unsafe {
                    std::slice::from_raw_parts(
                        data_b.as_ptr() as *const f32,
                        data_b.len(),
                    )
                };

                // Use scirs2_core's zero-allocation SIMD API
                f32::simd_add_into(self_f32, other_f32, &mut result_f32);

                Ok(())
            })
        })?;

        // Reinterpret Vec<f32> as Vec<T> (NO allocation - just transmute)
        let result_t = unsafe {
            let ptr = result_f32.as_mut_ptr() as *mut T;
            let len = result_f32.len();
            let cap = result_f32.capacity();
            std::mem::forget(result_f32);  // Prevent double-free
            Vec::from_raw_parts(ptr, len, cap)
        };

        // Create Tensor from transmuted buffer (NO allocation - just move)
        Self::from_data(result_t, self.shape().to_vec(), self.device)
    }

    /// ðŸš€ **Phase 3**: Optimal SIMD multiplication with uninit buffer + scirs2 API
    #[cfg(feature = "simd")]
    pub fn mul_op_simd_phase3(&self, other: &Self) -> Result<Self> {
        if self.shape() != other.shape() {
            return Err(TorshError::ShapeMismatch {
                expected: self.shape().to_vec(),
                got: other.shape().to_vec(),
            });
        }

        let len = self.numel();

        // Phase 2 improvement: Uninit buffer allocation (skip zero-initialization)
        let mut result_f32: Vec<f32> = Vec::with_capacity(len);
        unsafe { result_f32.set_len(len); }

        // Use zero-copy scoped access for inputs
        self.with_data_slice(|data_a| {
            other.with_data_slice(|data_b| {
                // Cast to f32 slices
                let self_f32 = unsafe {
                    std::slice::from_raw_parts(
                        data_a.as_ptr() as *const f32,
                        data_a.len(),
                    )
                };
                let other_f32 = unsafe {
                    std::slice::from_raw_parts(
                        data_b.as_ptr() as *const f32,
                        data_b.len(),
                    )
                };

                // Use scirs2_core's zero-allocation SIMD API
                f32::simd_mul_into(self_f32, other_f32, &mut result_f32);

                Ok(())
            })
        })?;

        // Reinterpret Vec<f32> as Vec<T>
        let result_t = unsafe {
            let ptr = result_f32.as_mut_ptr() as *mut T;
            let len = result_f32.len();
            let cap = result_f32.capacity();
            std::mem::forget(result_f32);
            Vec::from_raw_parts(ptr, len, cap)
        };

        Self::from_data(result_t, self.shape().to_vec(), self.device)
    }

    /// ðŸš€ **Phase 3**: Optimal SIMD subtraction with uninit buffer + scirs2 API
    #[cfg(feature = "simd")]
    pub fn sub_op_simd_phase3(&self, other: &Self) -> Result<Self> {
        if self.shape() != other.shape() {
            return Err(TorshError::ShapeMismatch {
                expected: self.shape().to_vec(),
                got: other.shape().to_vec(),
            });
        }

        let len = self.numel();
        let mut result_f32: Vec<f32> = Vec::with_capacity(len);
        unsafe { result_f32.set_len(len); }

        self.with_data_slice(|data_a| {
            other.with_data_slice(|data_b| {
                let self_f32 = unsafe {
                    std::slice::from_raw_parts(data_a.as_ptr() as *const f32, data_a.len())
                };
                let other_f32 = unsafe {
                    std::slice::from_raw_parts(data_b.as_ptr() as *const f32, data_b.len())
                };
                f32::simd_sub_into(self_f32, other_f32, &mut result_f32);
                Ok(())
            })
        })?;

        let result_t = unsafe {
            let ptr = result_f32.as_mut_ptr() as *mut T;
            let len = result_f32.len();
            let cap = result_f32.capacity();
            std::mem::forget(result_f32);
            Vec::from_raw_parts(ptr, len, cap)
        };

        Self::from_data(result_t, self.shape().to_vec(), self.device)
    }

    /// ðŸš€ **Phase 3**: Optimal SIMD division with uninit buffer + scirs2 API
    #[cfg(feature = "simd")]
    pub fn div_op_simd_phase3(&self, other: &Self) -> Result<Self> {
        if self.shape() != other.shape() {
            return Err(TorshError::ShapeMismatch {
                expected: self.shape().to_vec(),
                got: other.shape().to_vec(),
            });
        }

        let len = self.numel();
        let mut result_f32: Vec<f32> = Vec::with_capacity(len);
        unsafe { result_f32.set_len(len); }

        self.with_data_slice(|data_a| {
            other.with_data_slice(|data_b| {
                let self_f32 = unsafe {
                    std::slice::from_raw_parts(data_a.as_ptr() as *const f32, data_a.len())
                };
                let other_f32 = unsafe {
                    std::slice::from_raw_parts(data_b.as_ptr() as *const f32, data_b.len())
                };
                f32::simd_div_into(self_f32, other_f32, &mut result_f32);
                Ok(())
            })
        })?;

        let result_t = unsafe {
            let ptr = result_f32.as_mut_ptr() as *mut T;
            let len = result_f32.len();
            let cap = result_f32.capacity();
            std::mem::forget(result_f32);
            Vec::from_raw_parts(ptr, len, cap)
        };

        Self::from_data(result_t, self.shape().to_vec(), self.device)
    }

    /// Phase 3 fallback for add (when SIMD feature is disabled)
    #[cfg(not(feature = "simd"))]
    pub fn add_op_simd_phase3(&self, other: &Self) -> Result<Self> {
        self.element_wise_op(other, |a, b| a + b)
    }

    /// Phase 3 fallback for mul (when SIMD feature is disabled)
    #[cfg(not(feature = "simd"))]
    pub fn mul_op_simd_phase3(&self, other: &Self) -> Result<Self> {
        self.element_wise_op(other, |a, b| a * b)
    }

    /// Phase 3 fallback for sub (when SIMD feature is disabled)
    #[cfg(not(feature = "simd"))]
    pub fn sub_op_simd_phase3(&self, other: &Self) -> Result<Self> {
        self.element_wise_op(other, |a, b| a - b)
    }

    /// Phase 3 fallback for div (when SIMD feature is disabled)
    #[cfg(not(feature = "simd"))]
    pub fn div_op_simd_phase3(&self, other: &Self) -> Result<Self> {
        self.element_wise_op(other, |a, b| a / b)
    }

    // ============================================================================
    // PHASE 4: ADAPTIVE SIZE-BASED DISPATCH
    // ============================================================================
    // Intelligently routes operations based on tensor size:
    // - Small tensors (< 512): Scalar (SIMD overhead not worth it)
    // - Medium tensors (512 - 65536): Phase 3 SIMD
    // - Large tensors (> 65536): Parallel SIMD (Rayon)
    //
    // Expected: Optimal performance for ALL tensor sizes
    // ============================================================================

    /// Size thresholds for adaptive dispatch
    const SIMD_MIN_SIZE: usize = 512;       // Below this, scalar is faster
    const PARALLEL_MIN_SIZE: usize = 65536; // Above this, use parallel

    /// ðŸš€ **Phase 4+7**: Adaptive addition with size-based dispatch
    ///
    /// Automatically chooses the optimal strategy:
    /// - **< 512 elements**: Scalar (avoids SIMD overhead)
    /// - **512 - 65536 elements**: Phase 7 direct SIMD (if SimdOptimized storage)
    /// - **> 65536 elements**: Parallel SIMD with Rayon
    ///
    /// # Performance
    /// - Small tensors: Scalar performance (no overhead)
    /// - Medium tensors: Phase 7 direct SIMD (bypasses closures)
    /// - Large tensors: Parallel + SIMD for maximum throughput
    #[cfg(feature = "simd")]
    pub fn add_adaptive(&self, other: &Self) -> Result<Self> {
        let numel = self.numel();

        if numel < Self::SIMD_MIN_SIZE {
            // Small: Scalar path (SIMD overhead not worth it)
            self.element_wise_op(other, |a, b| a + b)
        } else if numel < Self::PARALLEL_MIN_SIZE {
            // Medium: Phase 7 direct SIMD (falls back to Phase 3 if not SimdOptimized)
            self.add_direct_simd(other)
        } else {
            // Large: Parallel SIMD
            self.add_parallel_simd(other)
        }
    }

    /// ðŸš€ **Phase 4+7**: Adaptive multiplication with size-based dispatch
    #[cfg(feature = "simd")]
    pub fn mul_adaptive(&self, other: &Self) -> Result<Self> {
        let numel = self.numel();

        if numel < Self::SIMD_MIN_SIZE {
            self.element_wise_op(other, |a, b| a * b)
        } else if numel < Self::PARALLEL_MIN_SIZE {
            // Medium: Phase 7 direct SIMD
            self.mul_direct_simd(other)
        } else {
            self.mul_parallel_simd(other)
        }
    }

    /// ðŸš€ **Phase 4+7**: Adaptive subtraction with size-based dispatch
    #[cfg(feature = "simd")]
    pub fn sub_adaptive(&self, other: &Self) -> Result<Self> {
        let numel = self.numel();

        if numel < Self::SIMD_MIN_SIZE {
            self.element_wise_op(other, |a, b| a - b)
        } else if numel < Self::PARALLEL_MIN_SIZE {
            self.sub_direct_simd(other)
        } else {
            self.sub_parallel_simd(other)
        }
    }

    /// ðŸš€ **Phase 4+7**: Adaptive division with size-based dispatch
    #[cfg(feature = "simd")]
    pub fn div_adaptive(&self, other: &Self) -> Result<Self> {
        let numel = self.numel();

        if numel < Self::SIMD_MIN_SIZE {
            self.element_wise_op(other, |a, b| a / b)
        } else if numel < Self::PARALLEL_MIN_SIZE {
            self.div_direct_simd(other)
        } else {
            self.div_parallel_simd(other)
        }
    }

    // ============================================================================
    // PHASE 7: DIRECT SLICE ACCESS SIMD (NO CLOSURES)
    // ============================================================================
    // Bypasses closure overhead by directly accessing SimdOptimized storage.
    // Falls back to Phase 3 for non-SimdOptimized storage types.
    //
    // Expected: Maximum SIMD performance by eliminating all abstraction overhead
    // ============================================================================

    /// ðŸš€ **Phase 7**: Direct SIMD addition bypassing closure overhead
    ///
    /// For SimdOptimized storage, accesses slices directly without closures.
    /// Falls back to Phase 3 for other storage types.
    ///
    /// # Performance
    /// - **SimdOptimized storage**: Zero closure overhead, direct SIMD
    /// - **Other storage**: Falls back to Phase 3 (with closure overhead)
    #[cfg(feature = "simd")]
    pub fn add_direct_simd(&self, other: &Self) -> Result<Self> {
        if self.shape() != other.shape() {
            return Err(TorshError::ShapeMismatch {
                expected: self.shape().to_vec(),
                got: other.shape().to_vec(),
            });
        }

        // Try direct slice access (only works for SimdOptimized storage)
        let self_slice = self.storage.try_as_slice_direct();
        let other_slice = other.storage.try_as_slice_direct();

        match (self_slice, other_slice) {
            (Some(a), Some(b)) => {
                // Both have direct access - maximum performance path
                let len = a.len();

                // Allocate uninit result buffer
                let mut result_f32: Vec<f32> = Vec::with_capacity(len);
                unsafe { result_f32.set_len(len); }

                // Cast to f32 slices (safe - we're Tensor<f32>)
                let self_f32 = unsafe {
                    std::slice::from_raw_parts(a.as_ptr() as *const f32, a.len())
                };
                let other_f32 = unsafe {
                    std::slice::from_raw_parts(b.as_ptr() as *const f32, b.len())
                };

                // Direct SIMD - no closures, no locks
                f32::simd_add_into(self_f32, other_f32, &mut result_f32);

                // Reinterpret Vec<f32> as Vec<T>
                let result_t = unsafe {
                    let ptr = result_f32.as_mut_ptr() as *mut T;
                    let len = result_f32.len();
                    let cap = result_f32.capacity();
                    std::mem::forget(result_f32);
                    Vec::from_raw_parts(ptr, len, cap)
                };

                // Use fast result tensor (skips ~10Âµs alignment copy)
                Ok(Self::from_data_fast(result_t, self.shape().to_vec(), self.device))
            }
            _ => {
                // Fall back to Phase 3 (with closure overhead)
                self.add_op_simd_phase3(other)
            }
        }
    }

    /// ðŸš€ **Phase 7**: Direct SIMD multiplication bypassing closure overhead
    #[cfg(feature = "simd")]
    pub fn mul_direct_simd(&self, other: &Self) -> Result<Self> {
        if self.shape() != other.shape() {
            return Err(TorshError::ShapeMismatch {
                expected: self.shape().to_vec(),
                got: other.shape().to_vec(),
            });
        }

        let self_slice = self.storage.try_as_slice_direct();
        let other_slice = other.storage.try_as_slice_direct();

        match (self_slice, other_slice) {
            (Some(a), Some(b)) => {
                let len = a.len();
                let mut result_f32: Vec<f32> = Vec::with_capacity(len);
                unsafe { result_f32.set_len(len); }

                let self_f32 = unsafe {
                    std::slice::from_raw_parts(a.as_ptr() as *const f32, a.len())
                };
                let other_f32 = unsafe {
                    std::slice::from_raw_parts(b.as_ptr() as *const f32, b.len())
                };

                f32::simd_mul_into(self_f32, other_f32, &mut result_f32);

                let result_t = unsafe {
                    let ptr = result_f32.as_mut_ptr() as *mut T;
                    let len = result_f32.len();
                    let cap = result_f32.capacity();
                    std::mem::forget(result_f32);
                    Vec::from_raw_parts(ptr, len, cap)
                };

                // Use fast result tensor (skips ~10Âµs alignment copy)
                Ok(Self::from_data_fast(result_t, self.shape().to_vec(), self.device))
            }
            _ => {
                self.mul_op_simd_phase3(other)
            }
        }
    }

    /// ðŸš€ **Phase 7**: Direct SIMD subtraction bypassing closure overhead
    #[cfg(feature = "simd")]
    pub fn sub_direct_simd(&self, other: &Self) -> Result<Self> {
        if self.shape() != other.shape() {
            return Err(TorshError::ShapeMismatch {
                expected: self.shape().to_vec(),
                got: other.shape().to_vec(),
            });
        }

        let self_slice = self.storage.try_as_slice_direct();
        let other_slice = other.storage.try_as_slice_direct();

        match (self_slice, other_slice) {
            (Some(a), Some(b)) => {
                let len = a.len();
                let mut result_f32: Vec<f32> = Vec::with_capacity(len);
                unsafe { result_f32.set_len(len); }

                let self_f32 = unsafe {
                    std::slice::from_raw_parts(a.as_ptr() as *const f32, a.len())
                };
                let other_f32 = unsafe {
                    std::slice::from_raw_parts(b.as_ptr() as *const f32, b.len())
                };

                f32::simd_sub_into(self_f32, other_f32, &mut result_f32);

                let result_t = unsafe {
                    let ptr = result_f32.as_mut_ptr() as *mut T;
                    let len = result_f32.len();
                    let cap = result_f32.capacity();
                    std::mem::forget(result_f32);
                    Vec::from_raw_parts(ptr, len, cap)
                };

                Ok(Self::from_data_fast(result_t, self.shape().to_vec(), self.device))
            }
            _ => {
                self.sub_op_simd_phase3(other)
            }
        }
    }

    /// ðŸš€ **Phase 7**: Direct SIMD division bypassing closure overhead
    #[cfg(feature = "simd")]
    pub fn div_direct_simd(&self, other: &Self) -> Result<Self> {
        if self.shape() != other.shape() {
            return Err(TorshError::ShapeMismatch {
                expected: self.shape().to_vec(),
                got: other.shape().to_vec(),
            });
        }

        let self_slice = self.storage.try_as_slice_direct();
        let other_slice = other.storage.try_as_slice_direct();

        match (self_slice, other_slice) {
            (Some(a), Some(b)) => {
                let len = a.len();
                let mut result_f32: Vec<f32> = Vec::with_capacity(len);
                unsafe { result_f32.set_len(len); }

                let self_f32 = unsafe {
                    std::slice::from_raw_parts(a.as_ptr() as *const f32, a.len())
                };
                let other_f32 = unsafe {
                    std::slice::from_raw_parts(b.as_ptr() as *const f32, b.len())
                };

                f32::simd_div_into(self_f32, other_f32, &mut result_f32);

                let result_t = unsafe {
                    let ptr = result_f32.as_mut_ptr() as *mut T;
                    let len = result_f32.len();
                    let cap = result_f32.capacity();
                    std::mem::forget(result_f32);
                    Vec::from_raw_parts(ptr, len, cap)
                };

                Ok(Self::from_data_fast(result_t, self.shape().to_vec(), self.device))
            }
            _ => {
                self.div_op_simd_phase3(other)
            }
        }
    }

    /// Parallel SIMD addition for large tensors (> 65K elements)
    ///
    /// Splits work into chunks and processes in parallel using Rayon,
    /// with SIMD processing within each chunk.
    #[cfg(feature = "simd")]
    pub fn add_parallel_simd(&self, other: &Self) -> Result<Self> {
        use scirs2_core::parallel_ops::*;

        if self.shape() != other.shape() {
            return Err(TorshError::ShapeMismatch {
                expected: self.shape().to_vec(),
                got: other.shape().to_vec(),
            });
        }

        let len = self.numel();
        let num_threads = num_cpus::get().max(1);
        let chunk_size = (len + num_threads - 1) / num_threads;

        // Allocate uninit result buffer
        let mut result_f32: Vec<f32> = Vec::with_capacity(len);
        unsafe { result_f32.set_len(len); }

        // Get input data
        self.with_data_slice(|data_a| {
            other.with_data_slice(|data_b| {
                let self_f32 = unsafe {
                    std::slice::from_raw_parts(
                        data_a.as_ptr() as *const f32,
                        data_a.len(),
                    )
                };
                let other_f32 = unsafe {
                    std::slice::from_raw_parts(
                        data_b.as_ptr() as *const f32,
                        data_b.len(),
                    )
                };

                // Process chunks in parallel
                let result_ptr = result_f32.as_mut_ptr();
                (0..num_threads).into_par_iter().for_each(|thread_idx| {
                    let start = thread_idx * chunk_size;
                    let end = (start + chunk_size).min(len);
                    if start < len {
                        let chunk_a = &self_f32[start..end];
                        let chunk_b = &other_f32[start..end];
                        let chunk_out = unsafe {
                            std::slice::from_raw_parts_mut(result_ptr.add(start), end - start)
                        };
                        f32::simd_add_into(chunk_a, chunk_b, chunk_out);
                    }
                });

                Ok(())
            })
        })?;

        // Reinterpret as Vec<T>
        let result_t = unsafe {
            let ptr = result_f32.as_mut_ptr() as *mut T;
            let len = result_f32.len();
            let cap = result_f32.capacity();
            std::mem::forget(result_f32);
            Vec::from_raw_parts(ptr, len, cap)
        };

        Self::from_data(result_t, self.shape().to_vec(), self.device)
    }

    /// Parallel SIMD multiplication for large tensors (> 65K elements)
    #[cfg(feature = "simd")]
    pub fn mul_parallel_simd(&self, other: &Self) -> Result<Self> {
        use scirs2_core::parallel_ops::*;

        if self.shape() != other.shape() {
            return Err(TorshError::ShapeMismatch {
                expected: self.shape().to_vec(),
                got: other.shape().to_vec(),
            });
        }

        let len = self.numel();
        let num_threads = num_cpus::get().max(1);
        let chunk_size = (len + num_threads - 1) / num_threads;

        // Allocate uninit result buffer
        let mut result_f32: Vec<f32> = Vec::with_capacity(len);
        unsafe { result_f32.set_len(len); }

        // Get input data
        self.with_data_slice(|data_a| {
            other.with_data_slice(|data_b| {
                let self_f32 = unsafe {
                    std::slice::from_raw_parts(
                        data_a.as_ptr() as *const f32,
                        data_a.len(),
                    )
                };
                let other_f32 = unsafe {
                    std::slice::from_raw_parts(
                        data_b.as_ptr() as *const f32,
                        data_b.len(),
                    )
                };

                // Process chunks in parallel
                let result_ptr = result_f32.as_mut_ptr();
                (0..num_threads).into_par_iter().for_each(|thread_idx| {
                    let start = thread_idx * chunk_size;
                    let end = (start + chunk_size).min(len);
                    if start < len {
                        let chunk_a = &self_f32[start..end];
                        let chunk_b = &other_f32[start..end];
                        let chunk_out = unsafe {
                            std::slice::from_raw_parts_mut(result_ptr.add(start), end - start)
                        };
                        f32::simd_mul_into(chunk_a, chunk_b, chunk_out);
                    }
                });

                Ok(())
            })
        })?;

        // Reinterpret as Vec<T>
        let result_t = unsafe {
            let ptr = result_f32.as_mut_ptr() as *mut T;
            let len = result_f32.len();
            let cap = result_f32.capacity();
            std::mem::forget(result_f32);
            Vec::from_raw_parts(ptr, len, cap)
        };

        Self::from_data(result_t, self.shape().to_vec(), self.device)
    }

    /// Parallel SIMD subtraction for large tensors (> 65K elements)
    #[cfg(feature = "simd")]
    pub fn sub_parallel_simd(&self, other: &Self) -> Result<Self> {
        use scirs2_core::parallel_ops::*;

        if self.shape() != other.shape() {
            return Err(TorshError::ShapeMismatch {
                expected: self.shape().to_vec(),
                got: other.shape().to_vec(),
            });
        }

        let len = self.numel();
        let num_threads = num_cpus::get().max(1);
        let chunk_size = (len + num_threads - 1) / num_threads;

        let mut result_f32: Vec<f32> = Vec::with_capacity(len);
        unsafe { result_f32.set_len(len); }

        self.with_data_slice(|data_a| {
            other.with_data_slice(|data_b| {
                let self_f32 = unsafe {
                    std::slice::from_raw_parts(data_a.as_ptr() as *const f32, data_a.len())
                };
                let other_f32 = unsafe {
                    std::slice::from_raw_parts(data_b.as_ptr() as *const f32, data_b.len())
                };

                let result_ptr = result_f32.as_mut_ptr();
                (0..num_threads).into_par_iter().for_each(|thread_idx| {
                    let start = thread_idx * chunk_size;
                    let end = (start + chunk_size).min(len);
                    if start < len {
                        let chunk_a = &self_f32[start..end];
                        let chunk_b = &other_f32[start..end];
                        let chunk_out = unsafe {
                            std::slice::from_raw_parts_mut(result_ptr.add(start), end - start)
                        };
                        f32::simd_sub_into(chunk_a, chunk_b, chunk_out);
                    }
                });

                Ok(())
            })
        })?;

        let result_t = unsafe {
            let ptr = result_f32.as_mut_ptr() as *mut T;
            let len = result_f32.len();
            let cap = result_f32.capacity();
            std::mem::forget(result_f32);
            Vec::from_raw_parts(ptr, len, cap)
        };

        Self::from_data(result_t, self.shape().to_vec(), self.device)
    }

    /// Parallel SIMD division for large tensors (> 65K elements)
    #[cfg(feature = "simd")]
    pub fn div_parallel_simd(&self, other: &Self) -> Result<Self> {
        use scirs2_core::parallel_ops::*;

        if self.shape() != other.shape() {
            return Err(TorshError::ShapeMismatch {
                expected: self.shape().to_vec(),
                got: other.shape().to_vec(),
            });
        }

        let len = self.numel();
        let num_threads = num_cpus::get().max(1);
        let chunk_size = (len + num_threads - 1) / num_threads;

        let mut result_f32: Vec<f32> = Vec::with_capacity(len);
        unsafe { result_f32.set_len(len); }

        self.with_data_slice(|data_a| {
            other.with_data_slice(|data_b| {
                let self_f32 = unsafe {
                    std::slice::from_raw_parts(data_a.as_ptr() as *const f32, data_a.len())
                };
                let other_f32 = unsafe {
                    std::slice::from_raw_parts(data_b.as_ptr() as *const f32, data_b.len())
                };

                let result_ptr = result_f32.as_mut_ptr();
                (0..num_threads).into_par_iter().for_each(|thread_idx| {
                    let start = thread_idx * chunk_size;
                    let end = (start + chunk_size).min(len);
                    if start < len {
                        let chunk_a = &self_f32[start..end];
                        let chunk_b = &other_f32[start..end];
                        let chunk_out = unsafe {
                            std::slice::from_raw_parts_mut(result_ptr.add(start), end - start)
                        };
                        f32::simd_div_into(chunk_a, chunk_b, chunk_out);
                    }
                });

                Ok(())
            })
        })?;

        let result_t = unsafe {
            let ptr = result_f32.as_mut_ptr() as *mut T;
            let len = result_f32.len();
            let cap = result_f32.capacity();
            std::mem::forget(result_f32);
            Vec::from_raw_parts(ptr, len, cap)
        };

        Self::from_data(result_t, self.shape().to_vec(), self.device)
    }

    /// Phase 4 fallback for add (when SIMD feature is disabled)
    #[cfg(not(feature = "simd"))]
    pub fn add_adaptive(&self, other: &Self) -> Result<Self> {
        self.element_wise_op(other, |a, b| a + b)
    }

    /// Phase 4 fallback for mul (when SIMD feature is disabled)
    #[cfg(not(feature = "simd"))]
    pub fn mul_adaptive(&self, other: &Self) -> Result<Self> {
        self.element_wise_op(other, |a, b| a * b)
    }

    /// Phase 4 fallback for sub (when SIMD feature is disabled)
    #[cfg(not(feature = "simd"))]
    pub fn sub_adaptive(&self, other: &Self) -> Result<Self> {
        self.element_wise_op(other, |a, b| a - b)
    }

    /// Phase 4 fallback for div (when SIMD feature is disabled)
    #[cfg(not(feature = "simd"))]
    pub fn div_adaptive(&self, other: &Self) -> Result<Self> {
        self.element_wise_op(other, |a, b| a / b)
    }

    /// Phase 4 fallback for parallel_simd add (when SIMD feature is disabled)
    #[cfg(not(feature = "simd"))]
    pub fn add_parallel_simd(&self, other: &Self) -> Result<Self> {
        self.element_wise_op(other, |a, b| a + b)
    }

    /// Phase 4 fallback for parallel_simd mul (when SIMD feature is disabled)
    #[cfg(not(feature = "simd"))]
    pub fn mul_parallel_simd(&self, other: &Self) -> Result<Self> {
        self.element_wise_op(other, |a, b| a * b)
    }

    /// Phase 4 fallback for parallel_simd sub (when SIMD feature is disabled)
    #[cfg(not(feature = "simd"))]
    pub fn sub_parallel_simd(&self, other: &Self) -> Result<Self> {
        self.element_wise_op(other, |a, b| a - b)
    }

    /// Phase 4 fallback for parallel_simd div (when SIMD feature is disabled)
    #[cfg(not(feature = "simd"))]
    pub fn div_parallel_simd(&self, other: &Self) -> Result<Self> {
        self.element_wise_op(other, |a, b| a / b)
    }

    /// ðŸš€ Hyperoptimized SIMD element-wise operation for f32 tensors (up to 14.17x speedup)
    /// Uses adaptive selection to automatically choose the best SIMD strategy based on array size
    ///
    /// NOTE: This is the OLD implementation with memory copies. Kept for backward compatibility.
    /// Prefer using `add_op_simd_f32_zero_copy()` or `mul_op_simd_f32_zero_copy()` instead.
    pub fn element_wise_op_simd_f32(&self, other: &Self, op: SimdOpType) -> Result<Self> {
        #[cfg(feature = "simd")]
        {
            use scirs2_core::ndarray::ArrayView1;

            if self.shape() != other.shape() {
                return Err(torsh_core::error::TorshError::ShapeMismatch {
                    expected: self.shape().to_vec(),
                    got: other.shape().to_vec(),
                });
            }

            // Only proceed if this is actually an f32 tensor
            if std::any::TypeId::of::<T>() != std::any::TypeId::of::<f32>() {
                return self.element_wise_op(other, |a, b| match op {
                    SimdOpType::Add => a + b,
                    SimdOpType::Sub => a - b,
                    SimdOpType::Mul => a * b,
                    SimdOpType::Div => a / b,
                    _ => a, // Fallback for unsupported ops
                });
            }

            let self_data = self.data()?;
            let other_data = other.data()?;

            // Cast to f32 for hyperoptimized SIMD operations
            let self_f32: &[f32] = unsafe {
                std::slice::from_raw_parts(
                    self_data.as_ptr() as *const f32,
                    self_data.len(),
                )
            };
            let other_f32: &[f32] = unsafe {
                std::slice::from_raw_parts(
                    other_data.as_ptr() as *const f32,
                    other_data.len(),
                )
            };

            // Create ArrayView1 for hyperoptimized SIMD functions
            let self_view = ArrayView1::from(self_f32);
            let other_view = ArrayView1::from(other_f32);

            // ðŸš€ Use adaptive hyperoptimized SIMD functions with automatic strategy selection
            let result_array = match op {
                SimdOpType::Add => adaptive_simd_add_f32(&self_view, &other_view),
                SimdOpType::Mul => adaptive_simd_mul_f32(&self_view, &other_view),
                SimdOpType::Div => adaptive_simd_div_f32(&self_view, &other_view),
                SimdOpType::Sub => {
                    // Subtraction uses add with negated second operand for SIMD efficiency
                    let neg_other: Vec<f32> = other_f32.iter().map(|&x| -x).collect();
                    let neg_other_view = ArrayView1::from(&neg_other);
                    adaptive_simd_add_f32(&self_view, &neg_other_view)
                },
                _ => {
                    // Fallback for unsupported SIMD operations (Min/Max)
                    return self.element_wise_op(other, |a, b| match op {
                        SimdOpType::Min => if a < b { a } else { b },
                        SimdOpType::Max => if a > b { a } else { b },
                        _ => a,
                    });
                }
            };

            // Convert result back to T type
            let result_vec: Vec<T> = result_array.to_vec().into_iter().map(|f| unsafe {
                std::mem::transmute_copy::<f32, T>(&f)
            }).collect();

            Self::from_data(result_vec, self.shape().to_vec(), self.device.clone())
        }

        #[cfg(not(feature = "simd"))]
        {
            // Fallback to regular element-wise operation
            self.element_wise_op(other, |a, b| match op {
                SimdOpType::Add => a + b,
                SimdOpType::Sub => a - b,
                SimdOpType::Mul => a * b,
                SimdOpType::Div => a / b,
                SimdOpType::Min => if a < b { a } else { b },
                SimdOpType::Max => if a > b { a } else { b },
            })
        }
    }

    /// Fallback for SIMD operations when SIMD feature is not enabled
    #[cfg(not(feature = "simd"))]
    pub fn element_wise_op_simd_fallback<F>(&self, other: &Self, op: F) -> Result<Self>
    where
        F: Fn(T, T) -> T + Send + Sync,
        T: Send + Sync,
    {
        self.element_wise_op(other, op)
    }
}