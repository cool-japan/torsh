//! Quick Start Guide for Modern Optimizers
//!
//! This example shows the simplest way to use each modern optimizer.
//!
//! Run with: cargo run --example quickstart_modern_optimizers

use parking_lot::RwLock;
use std::sync::Arc;
use torsh_core::error::Result;
use torsh_optim::prelude::{Lion, Optimizer, Prodigy, ScheduleFreeAdamW, Sophia};
use torsh_tensor::creation::randn;

fn main() -> Result<()> {
    println!("=== Quick Start: Modern Optimizers ===\n");

    // Create example parameters
    let param1 = Arc::new(RwLock::new(randn::<f32>(&[128, 256])?));
    let param2 = Arc::new(RwLock::new(randn::<f32>(&[256, 10])?));
    let params = vec![param1.clone(), param2.clone()];

    // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    // 1. Lion - Simple and Memory Efficient
    // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    println!("1ï¸âƒ£  Lion Optimizer (Memory-Efficient)");
    println!("   Usage: Use lr that's 10x smaller than Adam");
    println!();

    // Basic usage
    let mut lion = Lion::new(params.clone(), 1e-4, 0.9, 0.99, 0.01);

    // Or use builder pattern
    let mut lion_builder = Lion::builder()
        .params(params.clone())
        .lr(1e-4)
        .beta1(0.9)
        .beta2(0.99)
        .weight_decay(0.01)
        .build();

    println!("   âœ“ Created Lion optimizer");
    println!("   âœ“ Learning rate: {}", lion.get_lr()[0]);
    println!();

    // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    // 2. Sophia - For LLM Training
    // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    println!("2ï¸âƒ£  Sophia Optimizer (LLM-Optimized)");
    println!("   Usage: 2-3x speedup for transformer training");
    println!();

    let _sophia = Sophia::builder()
        .params(params.clone())
        .lr(5e-4) // Typical for transformers
        .beta1(0.96)
        .beta2(0.99)
        .gamma(1.0) // Clipping threshold
        .hessian_update_interval(10) // Update Hessian every 10 steps
        .weight_decay(0.1)
        .build();

    println!("   âœ“ Created Sophia optimizer");
    println!("   âœ“ Hessian updates every 10 steps");
    println!();

    // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    // 3. Schedule-Free AdamW - No Schedule Needed!
    // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    println!("3ï¸âƒ£  Schedule-Free AdamW (No Schedule!)");
    println!("   Usage: Set constant LR, no warmup/decay needed");
    println!();

    let mut schedule_free = ScheduleFreeAdamW::builder()
        .params(params.clone())
        .lr(1e-3) // Constant learning rate!
        .beta1(0.9)
        .beta2(0.999)
        .c(0.05) // Averaging coefficient
        .weight_decay(0.01)
        .build();

    // Important: Switch between train/eval modes
    schedule_free.train(); // For training
    println!("   âœ“ Created Schedule-Free optimizer");
    println!("   âœ“ In training mode: {}", schedule_free.is_training());
    println!("   â„¹ï¸  Use .eval() during evaluation");
    println!();

    // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    // 4. Prodigy - Zero Hyperparameter Tuning!
    // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    println!("4ï¸âƒ£  Prodigy Optimizer (Zero Tuning!)");
    println!("   Usage: Just use lr=1.0, it adapts automatically");
    println!();

    let prodigy = Prodigy::builder()
        .params(params.clone())
        .lr(1.0) // Yes, really! Just use 1.0
        .beta1(0.9)
        .beta2(0.999)
        .weight_decay(0.0)
        .build();

    println!("   âœ“ Created Prodigy optimizer");
    println!(
        "   âœ“ Learning rate: {} (will adapt automatically!)",
        prodigy.get_lr()[0]
    );
    println!("   âœ“ Initial d scale: {:.2e}", prodigy.get_d());
    println!();

    // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    // Typical Training Loop Pattern
    // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("ğŸ“š Typical Training Loop:");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!();
    println!("for epoch in 0..num_epochs {{");
    println!("    for batch in dataloader {{");
    println!("        // 1. Forward pass");
    println!("        let output = model.forward(&batch.data);");
    println!("        let loss = criterion(&output, &batch.labels);");
    println!();
    println!("        // 2. Backward pass (computes gradients)");
    println!("        loss.backward();");
    println!();
    println!("        // 3. Optimizer step");
    println!("        optimizer.step()?;");
    println!();
    println!("        // 4. Zero gradients");
    println!("        optimizer.zero_grad();");
    println!("    }}");
    println!("}}");
    println!();

    // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    // Special Features
    // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("âœ¨ Special Features:");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!();

    println!("ğŸ”„ State Dict Save/Load:");
    let state = lion.state_dict()?;
    println!("   Saved optimizer state: {:?}", state.optimizer_type);
    lion_builder.load_state_dict(state)?;
    println!("   âœ“ Loaded state successfully");
    println!();

    println!("âš™ï¸  Dynamic Learning Rate:");
    lion.set_lr(2e-4);
    println!("   âœ“ Changed learning rate to: {}", lion.get_lr()[0]);
    println!();

    println!("ğŸ“Š Prodigy Adaptation Info:");
    println!(
        "   Current effective LR: {:.6e}",
        prodigy.get_effective_lr()
    );
    println!("   D scale factor: {:.6e}", prodigy.get_d());
    println!();

    // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    // Recommendations
    // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("ğŸ’¡ Quick Recommendations:");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!();
    println!("ğŸš€ Starting a new project?");
    println!("   â†’ Try Prodigy first (lr=1.0, zero tuning)");
    println!();
    println!("ğŸƒ Need something fast and simple?");
    println!("   â†’ Use Lion (lr=1e-4, memory efficient)");
    println!();
    println!("ğŸ¤– Training large language models?");
    println!("   â†’ Use Sophia (lr=5e-4, 2-3x speedup)");
    println!();
    println!("ğŸ˜Œ Don't want to tune LR schedules?");
    println!("   â†’ Use Schedule-Free AdamW (lr=1e-3)");
    println!();

    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("âœ… Quick Start Complete!");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");

    Ok(())
}
