//! Comparison of Modern Optimizers (2023-2024)
//!
//! This example demonstrates the usage of cutting-edge optimizers:
//! - Lion: Memory-efficient evolved sign momentum
//! - Sophia: Second-order optimization for LLMs
//! - Schedule-Free AdamW: No LR schedule tuning
//! - Prodigy: Automatic LR adaptation
//!
//! Run with: cargo run --example modern_optimizers_comparison

use parking_lot::RwLock;
use std::sync::Arc;
use torsh_core::error::Result;
use torsh_optim::prelude::{Lion, Optimizer, Prodigy, ScheduleFreeAdamW, Sophia};
use torsh_tensor::Tensor;

/// Simulate a simple optimization problem for demonstration
fn optimize_with_optimizer<O: Optimizer>(
    mut optimizer: O,
    param: Arc<RwLock<Tensor>>,
    name: &str,
    steps: usize,
) -> Result<f32> {
    println!("\n=== Optimizing with {} ===", name);

    for step in 0..steps {
        // Simulate gradient computation: gradient = 2 * param (quadratic function)
        let grad = {
            let p = param.read();
            p.mul_scalar(2.0)?
        };

        param.write().set_grad(Some(grad));

        // Optimization step
        optimizer.step().expect("Optimization step failed");

        // Print progress every 20 steps
        if step % 20 == 0 {
            let loss = {
                let p = param.read();
                let p_val = p.to_vec()?[0];
                p_val * p_val // Loss = x^2
            };
            println!("Step {}: Loss = {:.6}", step, loss);
        }

        optimizer.zero_grad();
    }

    // Final loss
    let final_loss = {
        let p = param.read();
        let p_val = p.to_vec()?[0];
        p_val * p_val
    };

    println!("Final Loss: {:.6}", final_loss);
    Ok(final_loss)
}

fn main() -> Result<()> {
    println!("=== Modern Optimizers Comparison ===");
    println!("Solving a simple quadratic minimization problem: f(x) = x^2");
    println!("Starting point: x = 10.0, Target: x = 0.0\n");

    let steps = 100;

    // 1. Lion Optimizer (Google Research, 2023)
    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("ğŸ¦ LION OPTIMIZER");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Key Features:");
    println!("  â€¢ Memory-efficient (only stores momentum)");
    println!("  â€¢ Sign-based updates");
    println!("  â€¢ Typical LR: 1e-4 (10x smaller than Adam)");
    {
        let param = Arc::new(RwLock::new(Tensor::scalar(10.0)?));
        let params = vec![param.clone()];
        let optimizer = Lion::new(params, 1e-2, 0.9, 0.99, 0.0);
        optimize_with_optimizer(optimizer, param, "Lion", steps)?;
    }

    // 2. Sophia Optimizer (2023)
    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("ğŸ“ SOPHIA OPTIMIZER");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Key Features:");
    println!("  â€¢ Second-order with Hessian diagonal");
    println!("  â€¢ 2-3x speedup for LLM training");
    println!("  â€¢ Clipped updates for stability");
    {
        let param = Arc::new(RwLock::new(Tensor::scalar(10.0)?));
        let params = vec![param.clone()];
        let optimizer = Sophia::new(params, 5e-2, 0.96, 0.99, 1.0, 10, 0.0);
        optimize_with_optimizer(optimizer, param, "Sophia", steps)?;
    }

    // 3. Schedule-Free AdamW (2024)
    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("ğŸ“… SCHEDULE-FREE AdamW");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Key Features:");
    println!("  â€¢ NO learning rate schedule needed!");
    println!("  â€¢ Fast/slow parameter sequences");
    println!("  â€¢ Train/eval mode switching");
    {
        let param = Arc::new(RwLock::new(Tensor::scalar(10.0)?));
        let params = vec![param.clone()];
        let mut optimizer = ScheduleFreeAdamW::new(params, 1e-1, 0.9, 0.999, 0.05, 0.0);
        optimizer.train(); // Ensure in training mode
        optimize_with_optimizer(optimizer, param, "Schedule-Free AdamW", steps)?;
    }

    // 4. Prodigy Optimizer (2024)
    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("ğŸ”® PRODIGY OPTIMIZER");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Key Features:");
    println!("  â€¢ ZERO learning rate tuning!");
    println!("  â€¢ Just use lr=1.0 for everything");
    println!("  â€¢ Automatic adaptation");
    {
        let param = Arc::new(RwLock::new(Tensor::scalar(10.0)?));
        let params = vec![param.clone()];
        let optimizer = Prodigy::new(params, 1.0, 0.9, 0.999, 0.0);
        println!("Initial learning rate scale (d): {:.2e}", optimizer.get_d());
        optimize_with_optimizer(optimizer, param, "Prodigy", steps)?;
    }

    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("âœ¨ COMPARISON SUMMARY");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("\nWhen to use each optimizer:");
    println!("\nğŸ¦ Lion:");
    println!("  â€¢ When memory is constrained");
    println!("  â€¢ For large vision/language models");
    println!("  â€¢ When Adam/AdamW is working but you want better efficiency");
    println!("\nğŸ“ Sophia:");
    println!("  â€¢ For large language model pre-training");
    println!("  â€¢ When training transformers at scale");
    println!("  â€¢ When you can afford periodic Hessian updates");
    println!("\nğŸ“… Schedule-Free AdamW:");
    println!("  â€¢ When you don't want to tune LR schedules");
    println!("  â€¢ For general-purpose deep learning");
    println!("  â€¢ When you want simplicity without sacrificing performance");
    println!("\nğŸ”® Prodigy:");
    println!("  â€¢ When you're unsure about learning rate");
    println!("  â€¢ For rapid prototyping and experiments");
    println!("  â€¢ When you want zero hyperparameter tuning");

    Ok(())
}
