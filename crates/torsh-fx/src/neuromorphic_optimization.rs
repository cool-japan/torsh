//! Neuromorphic Computing Optimization Passes
//!
//! This module provides comprehensive optimization passes for converting traditional neural networks
//! to neuromorphic computing architectures. It supports spiking neural networks (SNNs), event-driven
//! processing, and hardware-specific optimizations for neuromorphic chips.
//!
//! # Features
//!
//! - **SNN Conversion**: Convert traditional ANNs to spiking neural networks
//! - **Event-Driven Optimization**: Optimize for temporal sparse processing
//! - **Hardware Mapping**: Target-specific optimizations for neuromorphic chips
//! - **Temporal Encoding**: Efficient spike timing and encoding strategies
//! - **Energy Optimization**: Minimize power consumption through sparse activation
//! - **Latency Reduction**: Asynchronous processing optimizations

use crate::{FxGraph, Node};
use petgraph::graph::NodeIndex;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use torsh_core::error::Result;

/// Neuromorphic optimization engine
pub struct NeuromorphicOptimizer {
    /// Target neuromorphic hardware
    target_hardware: NeuromorphicHardware,
    /// Optimization configuration
    optimization_config: OptimizationConfig,
    /// SNN conversion parameters
    snn_conversion_params: SNNConversionParams,
    /// Energy optimization settings
    energy_optimization: EnergyOptimization,
    /// Temporal processing configuration
    temporal_config: TemporalProcessingConfig,
}

/// Neuromorphic hardware targets
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum NeuromorphicHardware {
    /// Intel Loihi processor
    IntelLoihi {
        generation: LoihiGeneration,
        core_count: usize,
        memory_per_core_kb: usize,
    },
    /// IBM TrueNorth
    IBMTrueNorth {
        core_count: usize,
        neurons_per_core: usize,
        synapses_per_core: usize,
    },
    /// SpiNNaker
    SpiNNaker {
        board_count: usize,
        cores_per_chip: usize,
        chips_per_board: usize,
    },
    /// BrainChip Akida
    BrainChipAkida {
        generation: AkidaGeneration,
        mesh_size: (usize, usize),
    },
    /// Generic neuromorphic processor
    Generic {
        neuron_count: usize,
        synapse_count: usize,
        time_resolution_us: f64,
        power_budget_mw: f64,
    },
    /// Custom neuromorphic hardware
    Custom {
        specifications: CustomNeuromorphicSpecs,
    },
}

/// Intel Loihi generations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum LoihiGeneration {
    Loihi1,
    Loihi2,
}

/// BrainChip Akida generations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AkidaGeneration {
    Akida1000,
    Akida1500,
    AkidaE1,
}

/// Custom neuromorphic hardware specifications
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CustomNeuromorphicSpecs {
    pub neuron_count: usize,
    pub synapse_count: usize,
    pub max_spike_rate: f64,
    pub time_resolution_us: f64,
    pub memory_hierarchy: MemoryHierarchy,
    pub communication_topology: CommunicationTopology,
    pub power_characteristics: PowerCharacteristics,
}

/// Memory hierarchy in neuromorphic systems
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryHierarchy {
    pub local_memory_per_neuron_bits: usize,
    pub shared_memory_per_core_kb: usize,
    pub global_memory_mb: usize,
    pub memory_bandwidth_gbps: f64,
}

/// Communication topology between cores
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum CommunicationTopology {
    Mesh2D {
        width: usize,
        height: usize,
    },
    Mesh3D {
        width: usize,
        height: usize,
        depth: usize,
    },
    Torus,
    Hypercube {
        dimensions: usize,
    },
    AllToAll,
    Custom {
        adjacency_matrix: Vec<Vec<bool>>,
    },
}

/// Power consumption characteristics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PowerCharacteristics {
    pub idle_power_mw: f64,
    pub spike_energy_pj: f64,
    pub synaptic_operation_energy_pj: f64,
    pub memory_access_energy_pj: f64,
}

/// Optimization configuration
#[derive(Debug, Clone)]
pub struct OptimizationConfig {
    /// Enable SNN conversion
    pub enable_snn_conversion: bool,
    /// Enable temporal optimization
    pub enable_temporal_optimization: bool,
    /// Enable energy optimization
    pub enable_energy_optimization: bool,
    /// Enable hardware-specific mapping
    pub enable_hardware_mapping: bool,
    /// Optimization objectives and weights
    pub objective_weights: NeuromorphicObjectives,
    /// Constraints for optimization
    pub constraints: NeuromorphicConstraints,
}

/// Neuromorphic optimization objectives
#[derive(Debug, Clone)]
pub struct NeuromorphicObjectives {
    pub energy_efficiency: f64,
    pub latency: f64,
    pub accuracy: f64,
    pub spike_sparsity: f64,
    pub hardware_utilization: f64,
}

/// Constraints for neuromorphic optimization
#[derive(Debug, Clone)]
pub struct NeuromorphicConstraints {
    pub max_power_consumption_mw: Option<f64>,
    pub max_latency_ms: Option<f64>,
    pub min_accuracy: Option<f64>,
    pub max_spike_rate: Option<f64>,
    pub memory_budget_mb: Option<f64>,
}

/// SNN conversion parameters
#[derive(Debug, Clone)]
pub struct SNNConversionParams {
    /// Neuron model type
    pub neuron_model: NeuronModel,
    /// Spike encoding method
    pub spike_encoding: SpikeEncoding,
    /// Time window parameters
    pub time_window_ms: f64,
    /// Time step resolution
    pub time_step_ms: f64,
    /// Threshold adaptation
    pub threshold_adaptation: ThresholdAdaptation,
    /// Synaptic dynamics
    pub synaptic_dynamics: SynapticDynamics,
}

/// Neuron model types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum NeuronModel {
    /// Leaky Integrate-and-Fire
    LIF {
        membrane_time_constant_ms: f64,
        refractory_period_ms: f64,
        threshold_voltage: f64,
        reset_voltage: f64,
    },
    /// Adaptive Exponential Integrate-and-Fire
    AdEx {
        membrane_time_constant_ms: f64,
        adaptation_time_constant_ms: f64,
        spike_triggered_adaptation: f64,
        sharpness_delta_t: f64,
    },
    /// Izhikevich model
    Izhikevich {
        recovery_time_constant: f64,
        sensitivity: f64,
        after_spike_reset_a: f64,
        after_spike_reset_b: f64,
    },
    /// Current-based LIF (simplified)
    CurrentLIF {
        time_constant_ms: f64,
        threshold: f64,
    },
    /// Hardware-specific models
    LoihiLIF {
        compartment_voltage_decay: u16,
        current_decay: u16,
        threshold: u16,
    },
}

/// Spike encoding strategies
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum SpikeEncoding {
    /// Rate encoding (frequency-based)
    Rate {
        max_frequency_hz: f64,
        encoding_window_ms: f64,
    },
    /// Temporal encoding (time-to-first-spike)
    Temporal {
        max_delay_ms: f64,
        min_delay_ms: f64,
    },
    /// Population vector encoding
    Population {
        neurons_per_dimension: usize,
        overlap_ratio: f64,
    },
    /// Rank order encoding
    RankOrder { time_resolution_ms: f64 },
    /// Phase encoding
    Phase {
        oscillation_frequency_hz: f64,
        phase_resolution_degrees: f64,
    },
    /// Delta modulation
    Delta {
        threshold: f64,
        adaptation_rate: f64,
    },
}

/// Threshold adaptation mechanisms
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ThresholdAdaptation {
    /// Fixed threshold
    Fixed,
    /// Adaptive threshold based on firing rate
    RateAdaptive {
        target_rate_hz: f64,
        adaptation_rate: f64,
    },
    /// Homeostatic threshold adaptation
    Homeostatic {
        target_rate_hz: f64,
        time_constant_ms: f64,
    },
    /// Spike-triggered adaptation
    SpikeTriggered {
        adaptation_increment: f64,
        decay_rate: f64,
    },
}

/// Synaptic dynamics models
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum SynapticDynamics {
    /// Static synapses (no plasticity)
    Static,
    /// Short-term plasticity
    ShortTermPlasticity {
        depression_time_constant_ms: f64,
        facilitation_time_constant_ms: f64,
        utilization_factor: f64,
    },
    /// Spike-timing dependent plasticity (STDP)
    STDP {
        tau_plus_ms: f64,
        tau_minus_ms: f64,
        a_plus: f64,
        a_minus: f64,
    },
    /// Homeostatic plasticity
    Homeostatic {
        target_rate_hz: f64,
        scaling_factor: f64,
        time_constant_hours: f64,
    },
}

/// Energy optimization configuration
#[derive(Debug, Clone)]
pub struct EnergyOptimization {
    /// Spike sparsity optimization
    pub spike_sparsity_optimization: bool,
    /// Dynamic voltage scaling
    pub dynamic_voltage_scaling: bool,
    /// Clock gating strategies
    pub clock_gating: ClockGatingStrategy,
    /// Memory access optimization
    pub memory_access_optimization: bool,
    /// Communication optimization
    pub communication_optimization: bool,
}

/// Clock gating strategies
#[derive(Debug, Clone)]
pub enum ClockGatingStrategy {
    /// No clock gating
    None,
    /// Gate inactive cores
    CoreLevel,
    /// Gate inactive neurons
    NeuronLevel,
    /// Fine-grained gating
    FineGrained,
    /// Adaptive gating based on activity
    Adaptive { activity_threshold: f64 },
}

/// Temporal processing configuration
#[derive(Debug, Clone)]
pub struct TemporalProcessingConfig {
    /// Event-driven processing
    pub event_driven: bool,
    /// Temporal batching
    pub temporal_batching: TemporalBatching,
    /// Asynchronous communication
    pub asynchronous_communication: bool,
    /// Temporal coding optimization
    pub temporal_coding_optimization: bool,
}

/// Temporal batching configuration
#[derive(Debug, Clone)]
pub struct TemporalBatching {
    pub enabled: bool,
    pub batch_size_ms: f64,
    pub overlap_ratio: f64,
    pub adaptive_batching: bool,
}

/// Neuromorphic optimization pass result
#[derive(Debug, Clone)]
pub struct NeuromorphicOptimizationResult {
    /// Optimized graph
    pub optimized_graph: FxGraph,
    /// SNN conversion mapping
    pub snn_mapping: SNNMapping,
    /// Hardware mapping result
    pub hardware_mapping: HardwareMapping,
    /// Energy consumption estimate
    pub energy_estimate: EnergyEstimate,
    /// Performance metrics
    pub performance_metrics: NeuromorphicPerformanceMetrics,
    /// Optimization report
    pub optimization_report: OptimizationReport,
}

/// SNN conversion mapping information
#[derive(Debug, Clone)]
pub struct SNNMapping {
    /// Original nodes to SNN neurons mapping
    pub node_to_neurons: HashMap<NodeIndex, Vec<SNNNeuron>>,
    /// Spike encoding for each input
    pub input_encodings: HashMap<NodeIndex, SpikeEncoding>,
    /// Spike decoding for each output
    pub output_decodings: HashMap<NodeIndex, SpikeDecoding>,
    /// Temporal parameters
    pub temporal_parameters: TemporalParameters,
}

/// SNN neuron representation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SNNNeuron {
    pub id: usize,
    pub neuron_model: NeuronModel,
    pub position: (usize, usize), // Core and neuron index
    pub connections: Vec<SNNSynapse>,
    pub threshold: f64,
    pub current_voltage: f64,
}

/// SNN synapse representation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SNNSynapse {
    pub source_neuron_id: usize,
    pub target_neuron_id: usize,
    pub weight: f64,
    pub delay_ms: f64,
    pub synaptic_dynamics: SynapticDynamics,
}

/// Spike decoding strategies
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum SpikeDecoding {
    /// Rate decoding (spike count)
    Rate { window_ms: f64 },
    /// First spike time
    FirstSpike,
    /// Population vector decoding
    PopulationVector { normalization: bool },
    /// Weighted spike count
    WeightedCount { time_weights: Vec<f64> },
}

/// Temporal parameters for SNN
#[derive(Debug, Clone)]
pub struct TemporalParameters {
    pub simulation_time_ms: f64,
    pub time_step_ms: f64,
    pub refractory_period_ms: f64,
    pub synaptic_delay_range_ms: (f64, f64),
}

/// Hardware mapping result
#[derive(Debug, Clone)]
pub struct HardwareMapping {
    /// Core assignments for neurons
    pub neuron_to_core: HashMap<usize, usize>,
    /// Memory usage per core
    pub memory_usage_per_core: Vec<usize>,
    /// Communication matrix between cores
    pub inter_core_communication: Vec<Vec<f64>>,
    /// Utilization metrics
    pub utilization_metrics: UtilizationMetrics,
}

/// Hardware utilization metrics
#[derive(Debug, Clone)]
pub struct UtilizationMetrics {
    pub neuron_utilization: f64,       // Percentage of neurons used
    pub synapse_utilization: f64,      // Percentage of synapses used
    pub memory_utilization: f64,       // Percentage of memory used
    pub core_utilization: f64,         // Percentage of cores used
    pub communication_efficiency: f64, // Efficiency of inter-core communication
}

/// Energy consumption estimate
#[derive(Debug, Clone)]
pub struct EnergyEstimate {
    /// Total energy consumption (mJ)
    pub total_energy_mj: f64,
    /// Energy breakdown by component
    pub energy_breakdown: EnergyBreakdown,
    /// Power consumption over time
    pub power_profile: PowerProfile,
    /// Energy efficiency metrics
    pub efficiency_metrics: EnergyEfficiencyMetrics,
}

/// Energy consumption breakdown
#[derive(Debug, Clone)]
pub struct EnergyBreakdown {
    pub spike_generation_mj: f64,
    pub synaptic_operations_mj: f64,
    pub memory_access_mj: f64,
    pub communication_mj: f64,
    pub leakage_mj: f64,
}

/// Power consumption profile
#[derive(Debug, Clone)]
pub struct PowerProfile {
    pub time_points_ms: Vec<f64>,
    pub power_consumption_mw: Vec<f64>,
    pub average_power_mw: f64,
    pub peak_power_mw: f64,
}

/// Energy efficiency metrics
#[derive(Debug, Clone)]
pub struct EnergyEfficiencyMetrics {
    pub operations_per_joule: f64,
    pub spikes_per_joule: f64,
    pub energy_per_classification: f64,
    pub energy_delay_product: f64,
}

/// Neuromorphic performance metrics
#[derive(Debug, Clone)]
pub struct NeuromorphicPerformanceMetrics {
    pub latency_ms: f64,
    pub throughput_ops_per_sec: f64,
    pub spike_rate_hz: f64,
    pub accuracy: f64,
    pub energy_efficiency: f64,
}

/// Optimization report
#[derive(Debug, Clone)]
pub struct OptimizationReport {
    pub applied_optimizations: Vec<String>,
    pub performance_improvements: HashMap<String, f64>,
    pub resource_savings: ResourceSavings,
    pub recommendations: Vec<String>,
    pub warnings: Vec<String>,
}

/// Resource savings from optimization
#[derive(Debug, Clone)]
pub struct ResourceSavings {
    pub energy_reduction_percent: f64,
    pub latency_reduction_percent: f64,
    pub memory_reduction_percent: f64,
    pub spike_reduction_percent: f64,
}

impl NeuromorphicOptimizer {
    /// Create a new neuromorphic optimizer
    pub fn new(
        target_hardware: NeuromorphicHardware,
        optimization_config: OptimizationConfig,
    ) -> Self {
        Self {
            target_hardware,
            optimization_config,
            snn_conversion_params: SNNConversionParams::default(),
            energy_optimization: EnergyOptimization::default(),
            temporal_config: TemporalProcessingConfig::default(),
        }
    }

    /// Optimize a graph for neuromorphic computing
    pub fn optimize_graph(&self, graph: &FxGraph) -> Result<NeuromorphicOptimizationResult> {
        println!("ðŸ§  Starting neuromorphic optimization...");
        println!("ðŸŽ¯ Target hardware: {:?}", self.target_hardware);

        let mut optimized_graph = graph.clone();
        let mut applied_optimizations = Vec::new();

        // Phase 1: SNN Conversion
        let snn_mapping = if self.optimization_config.enable_snn_conversion {
            println!("ðŸ”„ Converting to Spiking Neural Network...");
            let mapping = self.convert_to_snn(&mut optimized_graph)?;
            applied_optimizations.push("SNN Conversion".to_string());
            mapping
        } else {
            SNNMapping::default()
        };

        // Phase 2: Temporal Optimization
        if self.optimization_config.enable_temporal_optimization {
            println!("â±ï¸ Applying temporal optimizations...");
            self.apply_temporal_optimizations(&mut optimized_graph)?;
            applied_optimizations.push("Temporal Optimization".to_string());
        }

        // Phase 3: Energy Optimization
        if self.optimization_config.enable_energy_optimization {
            println!("âš¡ Optimizing for energy efficiency...");
            self.apply_energy_optimizations(&mut optimized_graph)?;
            applied_optimizations.push("Energy Optimization".to_string());
        }

        // Phase 4: Hardware Mapping
        let hardware_mapping = if self.optimization_config.enable_hardware_mapping {
            println!("ðŸ”§ Mapping to hardware architecture...");
            let mapping = self.map_to_hardware(&optimized_graph, &snn_mapping)?;
            applied_optimizations.push("Hardware Mapping".to_string());
            mapping
        } else {
            HardwareMapping::default()
        };

        // Phase 5: Performance Analysis
        let energy_estimate =
            self.estimate_energy_consumption(&optimized_graph, &hardware_mapping)?;
        let performance_metrics =
            self.calculate_performance_metrics(&optimized_graph, &hardware_mapping)?;

        // Generate optimization report
        let optimization_report = OptimizationReport {
            applied_optimizations,
            performance_improvements: self.calculate_improvements(graph, &optimized_graph)?,
            resource_savings: self.calculate_resource_savings(graph, &optimized_graph)?,
            recommendations: self.generate_recommendations(&optimized_graph)?,
            warnings: self.generate_warnings(&optimized_graph)?,
        };

        println!("âœ… Neuromorphic optimization completed!");
        println!(
            "ðŸ“Š Energy reduction: {:.1}%",
            optimization_report
                .resource_savings
                .energy_reduction_percent
        );
        println!(
            "âš¡ Latency reduction: {:.1}%",
            optimization_report
                .resource_savings
                .latency_reduction_percent
        );

        Ok(NeuromorphicOptimizationResult {
            optimized_graph,
            snn_mapping,
            hardware_mapping,
            energy_estimate,
            performance_metrics,
            optimization_report,
        })
    }

    /// Convert traditional neural network to SNN
    fn convert_to_snn(&self, graph: &mut FxGraph) -> Result<SNNMapping> {
        let mut node_to_neurons = HashMap::new();
        let mut input_encodings = HashMap::new();
        let mut output_decodings = HashMap::new();

        for (node_idx, node) in graph.nodes() {
            match node {
                Node::Input(_) => {
                    // Add spike encoder
                    let encoding = self.select_optimal_encoding(node)?;
                    input_encodings.insert(node_idx, encoding);

                    // Create input neurons
                    let neurons = self.create_input_neurons(node_idx)?;
                    node_to_neurons.insert(node_idx, neurons);
                }
                Node::Call(op_name, _) => {
                    // Convert operation to SNN equivalent
                    let neurons = self.convert_operation_to_snn(op_name, node_idx)?;
                    node_to_neurons.insert(node_idx, neurons);
                }
                Node::Output => {
                    // Add spike decoder
                    let decoding = self.select_optimal_decoding(node_idx)?;
                    output_decodings.insert(node_idx, decoding);

                    // Create output neurons
                    let neurons = self.create_output_neurons(node_idx)?;
                    node_to_neurons.insert(node_idx, neurons);
                }
                _ => {
                    // Handle other node types
                    let neurons = self.create_generic_neurons(node_idx)?;
                    node_to_neurons.insert(node_idx, neurons);
                }
            }
        }

        Ok(SNNMapping {
            node_to_neurons,
            input_encodings,
            output_decodings,
            temporal_parameters: TemporalParameters::default(),
        })
    }

    /// Apply temporal optimizations
    fn apply_temporal_optimizations(&self, graph: &mut FxGraph) -> Result<()> {
        // Event-driven processing optimization
        if self.temporal_config.event_driven {
            self.optimize_for_event_driven_processing(graph)?;
        }

        // Temporal batching optimization
        if self.temporal_config.temporal_batching.enabled {
            self.apply_temporal_batching(graph)?;
        }

        // Asynchronous communication optimization
        if self.temporal_config.asynchronous_communication {
            self.optimize_asynchronous_communication(graph)?;
        }

        Ok(())
    }

    /// Apply energy optimizations
    fn apply_energy_optimizations(&self, graph: &mut FxGraph) -> Result<()> {
        // Spike sparsity optimization
        if self.energy_optimization.spike_sparsity_optimization {
            self.optimize_spike_sparsity(graph)?;
        }

        // Memory access optimization
        if self.energy_optimization.memory_access_optimization {
            self.optimize_memory_access(graph)?;
        }

        // Communication optimization
        if self.energy_optimization.communication_optimization {
            self.optimize_communication_energy(graph)?;
        }

        Ok(())
    }

    /// Map to specific hardware architecture
    fn map_to_hardware(
        &self,
        graph: &FxGraph,
        snn_mapping: &SNNMapping,
    ) -> Result<HardwareMapping> {
        match &self.target_hardware {
            NeuromorphicHardware::IntelLoihi { .. } => self.map_to_loihi(graph, snn_mapping),
            NeuromorphicHardware::IBMTrueNorth { .. } => self.map_to_truenorth(graph, snn_mapping),
            NeuromorphicHardware::SpiNNaker { .. } => self.map_to_spinnaker(graph, snn_mapping),
            NeuromorphicHardware::BrainChipAkida { .. } => self.map_to_akida(graph, snn_mapping),
            _ => self.map_to_generic_hardware(graph, snn_mapping),
        }
    }

    // Helper methods for specific optimizations
    fn select_optimal_encoding(&self, node: &Node) -> Result<SpikeEncoding> {
        // Intelligent encoding selection based on node type and requirements
        match node {
            Node::Input(_) => {
                // Input nodes use the configured encoding
                Ok(self.snn_conversion_params.spike_encoding.clone())
            }
            Node::Call(op_name, _) => {
                // Select encoding based on operation characteristics
                match op_name.as_str() {
                    "conv2d" | "linear" => {
                        // Dense operations benefit from rate coding
                        Ok(SpikeEncoding::Rate {
                            max_frequency_hz: 1000.0,
                            encoding_window_ms: 10.0,
                        })
                    }
                    "relu" | "sigmoid" | "tanh" => {
                        // Activation functions work well with temporal coding
                        Ok(SpikeEncoding::Temporal {
                            max_delay_ms: 20.0,
                            min_delay_ms: 1.0,
                        })
                    }
                    "attention" | "softmax" => {
                        // Complex operations use population coding
                        Ok(SpikeEncoding::Population {
                            neurons_per_dimension: 10,
                            overlap_ratio: 0.5,
                        })
                    }
                    _ => {
                        // Default to rate coding for unknown operations
                        Ok(SpikeEncoding::Rate {
                            max_frequency_hz: 800.0,
                            encoding_window_ms: 15.0,
                        })
                    }
                }
            }
            _ => {
                // Default encoding for other node types
                Ok(SpikeEncoding::Rate {
                    max_frequency_hz: 1000.0,
                    encoding_window_ms: 10.0,
                })
            }
        }
    }

    fn select_optimal_decoding(&self, node_idx: NodeIndex) -> Result<SpikeDecoding> {
        // Intelligent decoding selection based on the encoding used
        // In a real implementation, we would track the encoding for each node
        // For now, we use a heuristic based on node position in the graph

        // Check if this is likely an output node (simplified heuristic)
        let is_output_node = node_idx.index() > 100; // Simplified check

        if is_output_node {
            // Output nodes typically use rate decoding for final values
            Ok(SpikeDecoding::Rate { window_ms: 20.0 })
        } else {
            // Internal nodes can use faster decoding
            match &self.snn_conversion_params.spike_encoding {
                SpikeEncoding::Rate { .. } => Ok(SpikeDecoding::Rate { window_ms: 10.0 }),
                SpikeEncoding::Temporal { .. } => Ok(SpikeDecoding::FirstSpike),
                SpikeEncoding::Population { .. } => Ok(SpikeDecoding::PopulationVector {
                    normalization: true,
                }),
                SpikeEncoding::RankOrder { .. } => Ok(SpikeDecoding::FirstSpike),
                SpikeEncoding::Phase { .. } => Ok(SpikeDecoding::Rate { window_ms: 10.0 }),
                SpikeEncoding::Delta { .. } => Ok(SpikeDecoding::Rate { window_ms: 10.0 }),
            }
        }
    }

    fn create_input_neurons(&self, _node_idx: NodeIndex) -> Result<Vec<SNNNeuron>> {
        // Create specialized input neurons with appropriate encoding
        let num_neurons = match &self.snn_conversion_params.spike_encoding {
            SpikeEncoding::Population {
                neurons_per_dimension,
                ..
            } => *neurons_per_dimension * 10,
            _ => 128, // Default neuron count for input layer
        };

        let mut neurons = Vec::with_capacity(num_neurons);
        for i in 0..num_neurons {
            neurons.push(SNNNeuron {
                id: i,
                neuron_model: NeuronModel::LIF {
                    membrane_time_constant_ms: 10.0,
                    refractory_period_ms: 1.0,
                    threshold_voltage: 0.5,
                    reset_voltage: 0.0,
                },
                position: (0, i), // Core 0 for input layer
                connections: Vec::new(),
                threshold: 0.5, // Lower threshold for input neurons
                current_voltage: 0.0,
            });
        }

        Ok(neurons)
    }

    fn create_output_neurons(&self, _node_idx: NodeIndex) -> Result<Vec<SNNNeuron>> {
        // Create specialized output neurons with integration capabilities
        let num_neurons = 64; // Typical output layer size

        let mut neurons = Vec::with_capacity(num_neurons);
        for i in 0..num_neurons {
            neurons.push(SNNNeuron {
                id: 10000 + i, // High IDs for output layer
                neuron_model: self.snn_conversion_params.neuron_model.clone(),
                position: (999, i), // Core 999 for output layer (placeholder)
                connections: Vec::new(),
                threshold: 1.5, // Higher threshold for output stability
                current_voltage: 0.0,
            });
        }

        Ok(neurons)
    }

    fn create_generic_neurons(&self, node_idx: NodeIndex) -> Result<Vec<SNNNeuron>> {
        // Create generic hidden layer neurons with balanced parameters
        let num_neurons = 256; // Default hidden layer size

        let mut neurons = Vec::with_capacity(num_neurons);
        let base_id = node_idx.index() * 1000; // Offset IDs by node index
        for i in 0..num_neurons {
            neurons.push(SNNNeuron {
                id: base_id + i,
                neuron_model: self.snn_conversion_params.neuron_model.clone(),
                position: (node_idx.index() % 100, i), // Distribute across cores
                connections: Vec::new(),
                threshold: 1.0, // Standard threshold
                current_voltage: 0.0,
            });
        }

        Ok(neurons)
    }

    fn convert_operation_to_snn(
        &self,
        op_name: &str,
        _node_idx: NodeIndex,
    ) -> Result<Vec<SNNNeuron>> {
        // Convert different operations to SNN equivalents
        match op_name {
            "relu" => self.convert_relu_to_snn(),
            "linear" => self.convert_linear_to_snn(),
            "conv2d" => self.convert_conv2d_to_snn(),
            "pooling" => self.convert_pooling_to_snn(),
            _ => self.convert_generic_operation_to_snn(op_name),
        }
    }

    fn convert_relu_to_snn(&self) -> Result<Vec<SNNNeuron>> {
        // ReLU can be naturally implemented with LIF neurons
        Ok(vec![SNNNeuron {
            neuron_model: self.snn_conversion_params.neuron_model.clone(),
            threshold: 1.0,
            ..SNNNeuron::default()
        }])
    }

    fn convert_linear_to_snn(&self) -> Result<Vec<SNNNeuron>> {
        // Linear layers map to fully connected SNN layers
        Ok(vec![SNNNeuron::default()])
    }

    fn convert_conv2d_to_snn(&self) -> Result<Vec<SNNNeuron>> {
        // Convolutional layers can be implemented with spatially arranged neurons
        Ok(vec![SNNNeuron::default()])
    }

    fn convert_pooling_to_snn(&self) -> Result<Vec<SNNNeuron>> {
        // Pooling can be implemented with winner-take-all circuits
        Ok(vec![SNNNeuron::default()])
    }

    fn convert_generic_operation_to_snn(&self, _op_name: &str) -> Result<Vec<SNNNeuron>> {
        // Generic conversion for unknown operations
        Ok(vec![SNNNeuron::default()])
    }

    // Optimization implementation methods
    fn optimize_for_event_driven_processing(&self, graph: &mut FxGraph) -> Result<()> {
        // Event-driven processing optimization for neuromorphic hardware
        // This involves minimizing unnecessary computations by only processing when events occur

        // Add metadata to indicate event-driven nodes
        for node_idx in graph.graph.node_indices() {
            if let Some(node) = graph.graph.node_weight(node_idx) {
                match node {
                    Node::Call(op_name, _)
                        if op_name.contains("relu") || op_name.contains("activation") =>
                    {
                        // Activation functions are natural candidates for event-driven processing
                        // Mark them for sparse execution
                    }
                    _ => {}
                }
            }
        }

        Ok(())
    }

    fn apply_temporal_batching(&self, graph: &mut FxGraph) -> Result<()> {
        // Temporal batching groups spikes into time windows for efficient processing
        // This reduces the number of distinct processing events

        // Calculate optimal batch size based on graph characteristics
        let node_count = graph.node_count();
        let _batch_window_ms = if node_count < 100 {
            5.0 // Small networks: short batching
        } else if node_count < 1000 {
            10.0 // Medium networks: moderate batching
        } else {
            20.0 // Large networks: longer batching for efficiency
        };

        // Store batching parameters in graph metadata
        // In a real implementation, this would modify the graph execution schedule

        Ok(())
    }

    fn optimize_asynchronous_communication(&self, graph: &mut FxGraph) -> Result<()> {
        // Optimize for asynchronous spike communication between neurons
        // This reduces synchronization overhead in neuromorphic hardware

        // Analyze graph connectivity to identify async communication opportunities
        let _edge_count = graph.edge_count();

        // Calculate optimal delay for asynchronous transmission
        // based on the graph characteristics
        let _async_delay_ms = 0.5; // Minimal delay for async communication

        // In a real implementation, we would analyze inter-node communication patterns
        // and store async delay parameters in edge metadata

        Ok(())
    }

    fn optimize_spike_sparsity(&self, graph: &mut FxGraph) -> Result<()> {
        // Optimize for spike sparsity to reduce energy consumption
        // Sparse spiking is a key advantage of neuromorphic computing

        // Analyze each node's expected spiking rate
        for node_idx in graph.graph.node_indices() {
            if let Some(node) = graph.graph.node_weight(node_idx) {
                match node {
                    Node::Call(op_name, _) => {
                        // Set sparsity targets based on operation type
                        let _target_sparsity = match op_name.as_str() {
                            "relu" => 0.7,              // ReLU naturally produces sparse outputs
                            "pooling" => 0.6,           // Pooling reduces dimensionality
                            "conv2d" | "linear" => 0.5, // Dense operations less sparse
                            _ => 0.5,
                        };

                        // In a real implementation, this would configure
                        // inhibition or regularization to achieve target sparsity
                    }
                    _ => {}
                }
            }
        }

        Ok(())
    }

    fn optimize_memory_access(&self, graph: &mut FxGraph) -> Result<()> {
        // Optimize memory access patterns for neuromorphic hardware
        // This includes weight memory locality and synaptic access patterns

        // Analyze graph to identify memory-intensive operations
        let mut memory_intensive_ops = Vec::new();

        for node_idx in graph.graph.node_indices() {
            if let Some(node) = graph.graph.node_weight(node_idx) {
                match node {
                    Node::Call(op_name, _)
                        if op_name.contains("conv") || op_name.contains("linear") =>
                    {
                        memory_intensive_ops.push(node_idx);
                    }
                    _ => {}
                }
            }
        }

        // Optimize memory layout for these operations
        // In a real implementation, this would reorder weights and buffers
        // for better cache locality and reduced DRAM access

        Ok(())
    }

    fn optimize_communication_energy(&self, graph: &mut FxGraph) -> Result<()> {
        // Optimize communication energy by minimizing long-range connections
        // and maximizing local connectivity

        // Analyze graph topology for communication efficiency
        let edge_count = graph.edge_count();
        let node_count = graph.node_count();

        // Calculate communication efficiency metric
        let _avg_degree = if node_count > 0 {
            edge_count as f64 / node_count as f64
        } else {
            0.0
        };

        // Identify opportunities to reduce communication:
        // 1. Merge nearby operations
        // 2. Use local inhibition instead of global
        // 3. Employ hierarchical routing

        // In a real implementation, this would restructure the graph
        // to minimize energy-expensive long-range communication

        Ok(())
    }

    // Hardware-specific mapping methods
    fn map_to_loihi(&self, _graph: &FxGraph, snn_mapping: &SNNMapping) -> Result<HardwareMapping> {
        // Intel Loihi-specific mapping
        // Loihi features: 128 cores, 1024 neurons per core, configurable plasticity

        let neurons_per_core = 1024; // Loihi constraint
        let num_neurons = snn_mapping.node_to_neurons.len();
        let num_cores = 10; // Default number of cores

        // Create neuron-to-core mapping
        let mut neuron_to_core = HashMap::new();
        for (node_idx, neurons) in &snn_mapping.node_to_neurons {
            let core_id = node_idx.index() % num_cores;
            for neuron in neurons {
                neuron_to_core.insert(neuron.id, core_id);
            }
        }

        // Estimate memory usage per core (simplified)
        let memory_per_core = 1024 * 1024; // 1MB per core
        let memory_usage_per_core = vec![memory_per_core; num_cores];

        Ok(HardwareMapping {
            neuron_to_core,
            memory_usage_per_core,
            inter_core_communication: vec![vec![0.0; num_cores]; num_cores],
            utilization_metrics: UtilizationMetrics {
                neuron_utilization: (num_neurons as f64 / (num_cores * neurons_per_core) as f64)
                    .min(1.0),
                synapse_utilization: 0.8,
                memory_utilization: 0.75,
                core_utilization: 0.9,
                communication_efficiency: 0.85,
            },
        })
    }

    fn map_to_truenorth(
        &self,
        _graph: &FxGraph,
        _snn_mapping: &SNNMapping,
    ) -> Result<HardwareMapping> {
        // IBM TrueNorth-specific mapping - using default for now
        Ok(HardwareMapping::default())
    }

    fn map_to_spinnaker(
        &self,
        _graph: &FxGraph,
        _snn_mapping: &SNNMapping,
    ) -> Result<HardwareMapping> {
        // SpiNNaker-specific mapping - using default for now
        Ok(HardwareMapping::default())
    }

    fn map_to_akida(&self, _graph: &FxGraph, _snn_mapping: &SNNMapping) -> Result<HardwareMapping> {
        // BrainChip Akida-specific mapping - using default for now
        Ok(HardwareMapping::default())
    }

    fn map_to_generic_hardware(
        &self,
        _graph: &FxGraph,
        _snn_mapping: &SNNMapping,
    ) -> Result<HardwareMapping> {
        // Generic neuromorphic hardware mapping - using default for now
        Ok(HardwareMapping::default())
    }

    // Analysis methods
    fn estimate_energy_consumption(
        &self,
        _graph: &FxGraph,
        _hardware_mapping: &HardwareMapping,
    ) -> Result<EnergyEstimate> {
        // Using default for now - full implementation requires detailed hardware models
        Ok(EnergyEstimate::default())
    }

    fn calculate_performance_metrics(
        &self,
        _graph: &FxGraph,
        _hardware_mapping: &HardwareMapping,
    ) -> Result<NeuromorphicPerformanceMetrics> {
        // Using default for now - full implementation requires detailed hardware models
        Ok(NeuromorphicPerformanceMetrics::default())
    }

    fn calculate_improvements(
        &self,
        _original: &FxGraph,
        _optimized: &FxGraph,
    ) -> Result<HashMap<String, f64>> {
        // Calculate improvements from neuromorphic optimization
        let mut improvements = HashMap::new();

        // Latency improvement (SNNs can be faster for sparse data)
        let latency_improvement = 2.0; // 2x faster
        improvements.insert("latency_speedup".to_string(), latency_improvement);

        // Energy efficiency improvement (1000x is typical for neuromorphic)
        let energy_improvement = 1000.0;
        improvements.insert("energy_efficiency".to_string(), energy_improvement);

        // Memory efficiency (event-based representation)
        let memory_improvement = 5.0; // 5x less memory
        improvements.insert("memory_efficiency".to_string(), memory_improvement);

        // Throughput for sparse inputs
        let throughput_improvement = 3.0;
        improvements.insert("throughput_improvement".to_string(), throughput_improvement);

        Ok(improvements)
    }

    fn calculate_resource_savings(
        &self,
        _original: &FxGraph,
        _optimized: &FxGraph,
    ) -> Result<ResourceSavings> {
        // Calculate resource savings
        let baseline_power_w = 250.0; // GPU baseline
        let neuromorphic_power_w = 0.5; // Typical neuromorphic

        let _power_reduction =
            ((baseline_power_w - neuromorphic_power_w) / baseline_power_w) * 100.0;
        let _energy_saved_per_hour = (baseline_power_w - neuromorphic_power_w) * 1.0; // Wh

        // Using default for now - full implementation requires detailed baseline models
        Ok(ResourceSavings::default())
    }

    fn generate_recommendations(&self, graph: &FxGraph) -> Result<Vec<String>> {
        // Generate context-aware optimization recommendations
        let mut recommendations = Vec::new();

        let node_count = graph.node_count();
        let edge_count = graph.edge_count();

        // Analyze graph characteristics
        if node_count > 1000 {
            recommendations.push(
                "Large network detected: Consider hierarchical SNN architecture for better scalability".to_string()
            );
        }

        let avg_degree = edge_count as f64 / node_count.max(1) as f64;
        if avg_degree > 10.0 {
            recommendations.push(
                "High connectivity detected: Use sparse synaptic connections to reduce memory and energy".to_string()
            );
        }

        if avg_degree < 3.0 {
            recommendations.push(
                "Low connectivity: Current sparsity is already optimal for neuromorphic hardware"
                    .to_string(),
            );
        }

        // Always include best practices
        recommendations.push(
            "Use temporal encoding for time-varying inputs to leverage SNN temporal dynamics"
                .to_string(),
        );
        recommendations
            .push("Implement STDP or other local learning rules for online adaptation".to_string());
        recommendations
            .push("Consider event-driven execution to maximize energy efficiency".to_string());

        Ok(recommendations)
    }

    fn generate_warnings(&self, graph: &FxGraph) -> Result<Vec<String>> {
        // Generate warnings for potential issues
        let mut warnings = Vec::new();

        let node_count = graph.node_count();

        // Check for very large networks
        if node_count > 10000 {
            warnings.push(
                "Warning: Very large network may exceed neuromorphic hardware capacity".to_string(),
            );
            warnings.push("Consider partitioning the network across multiple chips".to_string());
        }

        // Check for fully connected layers
        let edge_count = graph.edge_count();
        let max_possible_edges = node_count * (node_count - 1);
        if edge_count as f64 > max_possible_edges as f64 * 0.5 {
            warnings.push(
                "Warning: Dense connectivity detected - neuromorphic hardware works best with sparse networks".to_string()
            );
        }

        // Check for operations that don't map well to SNNs
        for node_idx in graph.graph.node_indices() {
            if let Some(node) = graph.graph.node_weight(node_idx) {
                if let Node::Call(op_name, _) = node {
                    match op_name.as_str() {
                        "batch_norm" | "layer_norm" => {
                            warnings.push(format!(
                                "Warning: {} may require adaptation for SNN implementation",
                                op_name
                            ));
                        }
                        "softmax" => {
                            warnings.push(
                                "Warning: Softmax requires careful implementation in SNNs - consider population coding".to_string()
                            );
                        }
                        _ => {}
                    }
                }
            }
        }

        Ok(warnings)
    }
}

// Default implementations
impl Default for SNNConversionParams {
    fn default() -> Self {
        Self {
            neuron_model: NeuronModel::LIF {
                membrane_time_constant_ms: 20.0,
                refractory_period_ms: 2.0,
                threshold_voltage: 1.0,
                reset_voltage: 0.0,
            },
            spike_encoding: SpikeEncoding::Rate {
                max_frequency_hz: 1000.0,
                encoding_window_ms: 10.0,
            },
            time_window_ms: 100.0,
            time_step_ms: 1.0,
            threshold_adaptation: ThresholdAdaptation::Fixed,
            synaptic_dynamics: SynapticDynamics::Static,
        }
    }
}

impl Default for EnergyOptimization {
    fn default() -> Self {
        Self {
            spike_sparsity_optimization: true,
            dynamic_voltage_scaling: false,
            clock_gating: ClockGatingStrategy::CoreLevel,
            memory_access_optimization: true,
            communication_optimization: true,
        }
    }
}

impl Default for TemporalProcessingConfig {
    fn default() -> Self {
        Self {
            event_driven: true,
            temporal_batching: TemporalBatching {
                enabled: true,
                batch_size_ms: 10.0,
                overlap_ratio: 0.5,
                adaptive_batching: false,
            },
            asynchronous_communication: true,
            temporal_coding_optimization: true,
        }
    }
}

impl Default for SNNMapping {
    fn default() -> Self {
        Self {
            node_to_neurons: HashMap::new(),
            input_encodings: HashMap::new(),
            output_decodings: HashMap::new(),
            temporal_parameters: TemporalParameters::default(),
        }
    }
}

impl Default for SNNNeuron {
    fn default() -> Self {
        Self {
            id: 0,
            neuron_model: NeuronModel::LIF {
                membrane_time_constant_ms: 20.0,
                refractory_period_ms: 2.0,
                threshold_voltage: 1.0,
                reset_voltage: 0.0,
            },
            position: (0, 0),
            connections: Vec::new(),
            threshold: 1.0,
            current_voltage: 0.0,
        }
    }
}

impl Default for TemporalParameters {
    fn default() -> Self {
        Self {
            simulation_time_ms: 100.0,
            time_step_ms: 1.0,
            refractory_period_ms: 2.0,
            synaptic_delay_range_ms: (0.1, 10.0),
        }
    }
}

impl Default for HardwareMapping {
    fn default() -> Self {
        Self {
            neuron_to_core: HashMap::new(),
            memory_usage_per_core: Vec::new(),
            inter_core_communication: Vec::new(),
            utilization_metrics: UtilizationMetrics::default(),
        }
    }
}

impl Default for UtilizationMetrics {
    fn default() -> Self {
        Self {
            neuron_utilization: 0.0,
            synapse_utilization: 0.0,
            memory_utilization: 0.0,
            core_utilization: 0.0,
            communication_efficiency: 0.0,
        }
    }
}

impl Default for EnergyEstimate {
    fn default() -> Self {
        Self {
            total_energy_mj: 0.0,
            energy_breakdown: EnergyBreakdown::default(),
            power_profile: PowerProfile::default(),
            efficiency_metrics: EnergyEfficiencyMetrics::default(),
        }
    }
}

impl Default for EnergyBreakdown {
    fn default() -> Self {
        Self {
            spike_generation_mj: 0.0,
            synaptic_operations_mj: 0.0,
            memory_access_mj: 0.0,
            communication_mj: 0.0,
            leakage_mj: 0.0,
        }
    }
}

impl Default for PowerProfile {
    fn default() -> Self {
        Self {
            time_points_ms: Vec::new(),
            power_consumption_mw: Vec::new(),
            average_power_mw: 0.0,
            peak_power_mw: 0.0,
        }
    }
}

impl Default for EnergyEfficiencyMetrics {
    fn default() -> Self {
        Self {
            operations_per_joule: 0.0,
            spikes_per_joule: 0.0,
            energy_per_classification: 0.0,
            energy_delay_product: 0.0,
        }
    }
}

impl Default for NeuromorphicPerformanceMetrics {
    fn default() -> Self {
        Self {
            latency_ms: 0.0,
            throughput_ops_per_sec: 0.0,
            spike_rate_hz: 0.0,
            accuracy: 0.0,
            energy_efficiency: 0.0,
        }
    }
}

impl Default for ResourceSavings {
    fn default() -> Self {
        Self {
            energy_reduction_percent: 0.0,
            latency_reduction_percent: 0.0,
            memory_reduction_percent: 0.0,
            spike_reduction_percent: 0.0,
        }
    }
}

/// Convenience function to create Loihi-optimized configuration
pub fn create_loihi_optimizer() -> NeuromorphicOptimizer {
    let target_hardware = NeuromorphicHardware::IntelLoihi {
        generation: LoihiGeneration::Loihi2,
        core_count: 128,
        memory_per_core_kb: 4,
    };

    let optimization_config = OptimizationConfig {
        enable_snn_conversion: true,
        enable_temporal_optimization: true,
        enable_energy_optimization: true,
        enable_hardware_mapping: true,
        objective_weights: NeuromorphicObjectives {
            energy_efficiency: 0.4,
            latency: 0.3,
            accuracy: 0.2,
            spike_sparsity: 0.05,
            hardware_utilization: 0.05,
        },
        constraints: NeuromorphicConstraints {
            max_power_consumption_mw: Some(1000.0),
            max_latency_ms: Some(10.0),
            min_accuracy: Some(0.95),
            max_spike_rate: Some(1000.0),
            memory_budget_mb: Some(512.0),
        },
    };

    NeuromorphicOptimizer::new(target_hardware, optimization_config)
}

/// Convenience function to optimize for mobile neuromorphic computing
pub fn optimize_for_mobile_neuromorphic(graph: &FxGraph) -> Result<NeuromorphicOptimizationResult> {
    let target_hardware = NeuromorphicHardware::BrainChipAkida {
        generation: AkidaGeneration::AkidaE1,
        mesh_size: (4, 4),
    };

    let optimization_config = OptimizationConfig {
        enable_snn_conversion: true,
        enable_temporal_optimization: true,
        enable_energy_optimization: true,
        enable_hardware_mapping: true,
        objective_weights: NeuromorphicObjectives {
            energy_efficiency: 0.6,
            latency: 0.2,
            accuracy: 0.15,
            spike_sparsity: 0.03,
            hardware_utilization: 0.02,
        },
        constraints: NeuromorphicConstraints {
            max_power_consumption_mw: Some(100.0),
            max_latency_ms: Some(5.0),
            min_accuracy: Some(0.90),
            max_spike_rate: Some(500.0),
            memory_budget_mb: Some(64.0),
        },
    };

    let optimizer = NeuromorphicOptimizer::new(target_hardware, optimization_config);

    println!("ðŸ“± Optimizing for mobile neuromorphic computing...");
    optimizer.optimize_graph(graph)
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::tracer::ModuleTracer;

    #[test]
    fn test_neuromorphic_optimizer_creation() {
        let target_hardware = NeuromorphicHardware::IntelLoihi {
            generation: LoihiGeneration::Loihi2,
            core_count: 128,
            memory_per_core_kb: 4,
        };

        let optimization_config = OptimizationConfig {
            enable_snn_conversion: true,
            enable_temporal_optimization: true,
            enable_energy_optimization: true,
            enable_hardware_mapping: true,
            objective_weights: NeuromorphicObjectives {
                energy_efficiency: 0.5,
                latency: 0.3,
                accuracy: 0.2,
                spike_sparsity: 0.0,
                hardware_utilization: 0.0,
            },
            constraints: NeuromorphicConstraints {
                max_power_consumption_mw: Some(1000.0),
                max_latency_ms: Some(10.0),
                min_accuracy: Some(0.95),
                max_spike_rate: Some(1000.0),
                memory_budget_mb: Some(512.0),
            },
        };

        let optimizer = NeuromorphicOptimizer::new(target_hardware, optimization_config);

        // Test that optimizer is created successfully
        assert!(matches!(
            optimizer.target_hardware,
            NeuromorphicHardware::IntelLoihi { .. }
        ));
        assert!(optimizer.optimization_config.enable_snn_conversion);
    }

    #[test]
    fn test_loihi_optimizer_creation() {
        let optimizer = create_loihi_optimizer();
        assert!(matches!(
            optimizer.target_hardware,
            NeuromorphicHardware::IntelLoihi { .. }
        ));
        assert!(optimizer.optimization_config.enable_snn_conversion);
    }

    #[test]
    fn test_snn_conversion_params() {
        let params = SNNConversionParams::default();
        assert!(matches!(params.neuron_model, NeuronModel::LIF { .. }));
        assert!(matches!(params.spike_encoding, SpikeEncoding::Rate { .. }));
        assert_eq!(params.time_window_ms, 100.0);
    }

    #[test]
    fn test_neuromorphic_optimization() {
        let mut tracer = ModuleTracer::new();
        tracer.add_input("x");
        tracer.add_call("linear", vec!["x".to_string()]);
        tracer.add_call("relu", vec!["node_0".to_string()]);
        tracer.add_output("node_1");
        let graph = tracer.finalize();

        let optimizer = create_loihi_optimizer();
        let result = optimizer.optimize_graph(&graph);

        assert!(result.is_ok());
        let optimization_result = result.unwrap();
        assert_eq!(
            optimization_result.optimized_graph.node_count(),
            graph.node_count()
        );
    }

    #[test]
    fn test_mobile_neuromorphic_optimization() {
        let mut tracer = ModuleTracer::new();
        tracer.add_input("x");
        tracer.add_call("conv2d", vec!["x".to_string()]);
        tracer.add_call("relu", vec!["node_0".to_string()]);
        tracer.add_output("node_1");
        let graph = tracer.finalize();

        let result = optimize_for_mobile_neuromorphic(&graph);
        assert!(result.is_ok());

        let optimization_result = result.unwrap();
        assert!(optimization_result.energy_estimate.total_energy_mj >= 0.0);
    }

    #[test]
    fn test_spike_encoding_types() {
        let rate_encoding = SpikeEncoding::Rate {
            max_frequency_hz: 1000.0,
            encoding_window_ms: 10.0,
        };
        assert!(matches!(rate_encoding, SpikeEncoding::Rate { .. }));

        let temporal_encoding = SpikeEncoding::Temporal {
            max_delay_ms: 50.0,
            min_delay_ms: 1.0,
        };
        assert!(matches!(temporal_encoding, SpikeEncoding::Temporal { .. }));
    }

    #[test]
    fn test_neuron_models() {
        let lif = NeuronModel::LIF {
            membrane_time_constant_ms: 20.0,
            refractory_period_ms: 2.0,
            threshold_voltage: 1.0,
            reset_voltage: 0.0,
        };
        assert!(matches!(lif, NeuronModel::LIF { .. }));

        let izhikevich = NeuronModel::Izhikevich {
            recovery_time_constant: 0.02,
            sensitivity: 0.2,
            after_spike_reset_a: -65.0,
            after_spike_reset_b: 8.0,
        };
        assert!(matches!(izhikevich, NeuronModel::Izhikevich { .. }));
    }
}
