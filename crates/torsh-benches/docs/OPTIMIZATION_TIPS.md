# ToRSh Performance Optimization Tips

## Overview

This guide provides practical optimization strategies to maximize performance when using ToRSh for machine learning workloads. The recommendations are organized by optimization scope and impact level.

## Quick Wins (High Impact, Low Effort)

### 1. **Optimal Batch Sizes**

#### Finding the Sweet Spot
```rust
use torsh_benches::utils::BatchSizeOptimizer;

// Automatically find optimal batch size
let optimizer = BatchSizeOptimizer::new()
    .with_model(your_model)
    .with_memory_limit(0.9) // Use 90% of available memory
    .with_latency_constraint(Duration::from_millis(10));

let optimal_batch = optimizer.find_optimal_batch_size()?;
println!("Optimal batch size: {}", optimal_batch);
```

**Rules of Thumb:**
- **GPU**: Start with powers of 2 (16, 32, 64, 128)
- **CPU**: Match the number of cores or logical threads
- **Memory-bound**: Largest batch that fits in memory
- **Latency-critical**: Smallest batch that maintains throughput

#### Batch Size Impact Analysis
```rust
// Measure performance across different batch sizes
let batch_sizes = vec![1, 2, 4, 8, 16, 32, 64, 128, 256];
let mut results = Vec::new();

for batch_size in batch_sizes {
    let throughput = measure_inference_throughput(model, batch_size)?;
    let latency = measure_single_inference_latency(model, batch_size)?;
    let memory = measure_peak_memory_usage(model, batch_size)?;
    
    results.push(BatchResult {
        batch_size,
        throughput,
        latency,
        memory,
        efficiency: throughput / latency.as_secs_f64(),
    });
}

// Find knee point in throughput curve
let optimal = find_knee_point(&results);
```

### 2. **Data Type Optimization**

#### Mixed Precision Training
```rust
use torsh_autograd::GradScaler;
use torsh_tensor::DType;

// Enable automatic mixed precision
let scaler = GradScaler::new();
let mut model = model.to_dtype(DType::F16);

for (inputs, targets) in dataloader {
    // Forward pass in FP16
    let outputs = model.forward(&inputs.to_dtype(DType::F16))?;
    let loss = criterion(&outputs, &targets);
    
    // Scale loss to prevent gradient underflow
    let scaled_loss = scaler.scale(&loss);
    
    // Backward pass
    scaled_loss.backward()?;
    
    // Unscale gradients and update
    scaler.unscale_(&mut optimizer);
    scaler.step(&mut optimizer);
    scaler.update();
}
```

#### Quantization for Inference
```rust
use torsh_quantization::{quantize_model, QuantizationConfig};

// Post-training quantization
let quantization_config = QuantizationConfig::int8()
    .with_calibration_data(calibration_dataset)
    .with_accuracy_threshold(0.99); // Maintain 99% accuracy

let quantized_model = quantize_model(&model, &quantization_config)?;

// Measure performance improvement
let fp32_time = benchmark_inference(&model, &test_data)?;
let int8_time = benchmark_inference(&quantized_model, &test_data)?;
println!("Speedup: {:.2}x", fp32_time.as_secs_f64() / int8_time.as_secs_f64());
```

### 3. **Memory Management**

#### Pre-allocation Strategy
```rust
// Pre-allocate tensors to avoid runtime allocation
struct PreallocatedBuffers {
    intermediate_1: Tensor,
    intermediate_2: Tensor,
    output_buffer: Tensor,
}

impl PreallocatedBuffers {
    fn new(batch_size: usize, feature_dim: usize) -> Self {
        Self {
            intermediate_1: Tensor::zeros(vec![batch_size, feature_dim]),
            intermediate_2: Tensor::zeros(vec![batch_size, feature_dim * 2]),
            output_buffer: Tensor::zeros(vec![batch_size, 10]),
        }
    }
    
    fn reset(&mut self) {
        self.intermediate_1.zero_();
        self.intermediate_2.zero_();
        self.output_buffer.zero_();
    }
}

// Reuse buffers across iterations
let mut buffers = PreallocatedBuffers::new(batch_size, 512);
for batch in dataloader {
    buffers.reset();
    // Use buffers.intermediate_1, etc. for computations
}
```

#### Memory Pool Management
```rust
use torsh_backend::memory::MemoryPool;

// Configure memory pool for efficient allocation
let memory_pool = MemoryPool::new()
    .with_initial_size(2_000_000_000) // 2GB initial pool
    .with_growth_factor(1.5)
    .with_max_size(8_000_000_000)    // 8GB maximum
    .with_fragmentation_threshold(0.15);

// Set as default allocator
MemoryPool::set_default(memory_pool);
```

## GPU Optimization Strategies

### 1. **CUDA Kernel Optimization**

#### Optimal Thread Block Sizes
```rust
use torsh_backend::cuda::KernelLauncher;

// Auto-tune kernel parameters
let launcher = KernelLauncher::new("my_kernel")
    .auto_tune_block_size(&input_tensors)?;

// Manual optimization for specific patterns
let block_size = match operation_type {
    OperationType::ElementWise => (256, 1, 1),    // High thread count
    OperationType::Reduction => (128, 1, 1),      // Balanced
    OperationType::MatMul => (16, 16, 1),         // 2D block
};

launcher.set_block_size(block_size).launch(&kernel_args)?;
```

#### Memory Coalescing
```rust
// Bad: Strided memory access
for i in 0..batch_size {
    for j in (0..feature_dim).step_by(stride) {
        result[i] += input[i * feature_dim + j];
    }
}

// Good: Coalesced memory access
for j in 0..feature_dim {
    for i in 0..batch_size {
        result[i] += input[i * feature_dim + j];
    }
}
```

### 2. **Tensor Core Utilization**

#### Enabling Tensor Cores
```rust
use torsh_backend::cuda::TensorCoreConfig;

// Configure for optimal Tensor Core usage
let config = TensorCoreConfig::new()
    .enable_tensor_cores(true)
    .prefer_tensor_core_shapes(true)
    .optimize_for_throughput(true);

// Ensure shapes are Tensor Core friendly
fn optimize_shapes_for_tensor_cores(shapes: &[Vec<usize>]) -> Vec<Vec<usize>> {
    shapes.iter().map(|shape| {
        shape.iter().map(|&dim| {
            // Round up to nearest multiple of 8 for FP16
            (dim + 7) / 8 * 8
        }).collect()
    }).collect()
}
```

#### Mixed Precision with Tensor Cores
```rust
// Optimal matrix multiplication with Tensor Cores
fn optimized_matmul(a: &Tensor, b: &Tensor) -> Result<Tensor> {
    // Ensure inputs are FP16 and shapes are compatible
    let a_fp16 = a.to_dtype(DType::F16)?;
    let b_fp16 = b.to_dtype(DType::F16)?;
    
    // Pad shapes if necessary for Tensor Core alignment
    let (a_padded, b_padded) = pad_for_tensor_cores(&a_fp16, &b_fp16)?;
    
    // Perform matrix multiplication using Tensor Cores
    let result_fp16 = a_padded.matmul(&b_padded)?;
    
    // Convert back to FP32 if needed
    result_fp16.to_dtype(DType::F32)
}
```

### 3. **GPU Memory Optimization**

#### Gradient Checkpointing
```rust
use torsh_autograd::checkpoint;

// Trade computation for memory
fn forward_with_checkpointing(input: &Tensor, model: &Model) -> Result<Tensor> {
    let mut x = input.clone();
    
    // Checkpoint every few layers
    for (i, layer) in model.layers.iter().enumerate() {
        if i % 4 == 0 {
            x = checkpoint::checkpoint(|| layer.forward(&x))?;
        } else {
            x = layer.forward(&x)?;
        }
    }
    
    Ok(x)
}
```

#### Memory-Efficient Attention
```rust
use torsh_nn::attention::FlashAttention;

// Use memory-efficient attention implementation
let flash_attention = FlashAttention::new(
    embed_dim,
    num_heads,
    block_size: 64, // Smaller blocks for memory efficiency
)?;

// Standard attention: O(nÂ²) memory
let standard_output = standard_attention(query, key, value)?;

// Flash attention: O(n) memory
let flash_output = flash_attention.forward(query, key, value)?;
```

## CPU Optimization Strategies

### 1. **SIMD Vectorization**

#### Automatic Vectorization
```rust
// Ensure compiler can vectorize loops
#[inline(always)]
fn vectorized_elementwise_add(a: &[f32], b: &[f32], result: &mut [f32]) {
    assert_eq!(a.len(), b.len());
    assert_eq!(a.len(), result.len());
    
    // Compiler can auto-vectorize this loop
    for i in 0..a.len() {
        result[i] = a[i] + b[i];
    }
}
```

#### Manual SIMD Optimization
```rust
use std::simd::{f32x8, Simd};

fn simd_elementwise_add(a: &[f32], b: &[f32], result: &mut [f32]) {
    let simd_len = a.len() / 8;
    let remainder = a.len() % 8;
    
    // Process 8 elements at a time using SIMD
    for i in 0..simd_len {
        let offset = i * 8;
        let a_simd = Simd::<f32, 8>::from_slice(&a[offset..offset + 8]);
        let b_simd = Simd::<f32, 8>::from_slice(&b[offset..offset + 8]);
        let result_simd = a_simd + b_simd;
        result_simd.copy_to_slice(&mut result[offset..offset + 8]);
    }
    
    // Handle remaining elements
    for i in simd_len * 8..a.len() {
        result[i] = a[i] + b[i];
    }
}
```

### 2. **Cache Optimization**

#### Cache-Friendly Memory Access
```rust
// Bad: Cache-unfriendly matrix multiplication
fn naive_matmul(a: &[Vec<f32>], b: &[Vec<f32>]) -> Vec<Vec<f32>> {
    let n = a.len();
    let mut result = vec![vec![0.0; n]; n];
    
    for i in 0..n {
        for j in 0..n {
            for k in 0..n {
                result[i][j] += a[i][k] * b[k][j]; // Poor cache locality for b[k][j]
            }
        }
    }
    result
}

// Good: Cache-optimized with blocking
fn blocked_matmul(a: &[Vec<f32>], b: &[Vec<f32>], block_size: usize) -> Vec<Vec<f32>> {
    let n = a.len();
    let mut result = vec![vec![0.0; n]; n];
    
    for i in (0..n).step_by(block_size) {
        for j in (0..n).step_by(block_size) {
            for k in (0..n).step_by(block_size) {
                // Process block
                for ii in i..std::cmp::min(i + block_size, n) {
                    for jj in j..std::cmp::min(j + block_size, n) {
                        for kk in k..std::cmp::min(k + block_size, n) {
                            result[ii][jj] += a[ii][kk] * b[kk][jj];
                        }
                    }
                }
            }
        }
    }
    result
}
```

#### Data Layout Optimization
```rust
// Structure of Arrays (SoA) for better vectorization
#[derive(Clone)]
struct ParticlesSoA {
    x: Vec<f32>,
    y: Vec<f32>,
    z: Vec<f32>,
    mass: Vec<f32>,
}

// Array of Structures (AoS) - cache-unfriendly for bulk operations
#[derive(Clone)]
struct Particle {
    x: f32,
    y: f32,
    z: f32,
    mass: f32,
}

// SoA enables better SIMD usage
fn update_positions_soa(particles: &mut ParticlesSoA, dt: f32) {
    for i in 0..particles.x.len() {
        particles.x[i] += particles.vx[i] * dt; // Vectorizable
        particles.y[i] += particles.vy[i] * dt;
        particles.z[i] += particles.vz[i] * dt;
    }
}
```

### 3. **Thread Pool Optimization**

#### Optimal Thread Configuration
```rust
use rayon::prelude::*;
use std::thread;

// Configure thread pool based on workload
fn configure_thread_pool() {
    let num_cpus = thread::available_parallelism().unwrap().get();
    
    // For CPU-bound tasks: use all cores
    rayon::ThreadPoolBuilder::new()
        .num_threads(num_cpus)
        .build_global()
        .unwrap();
    
    // For I/O-bound tasks: oversubscribe
    rayon::ThreadPoolBuilder::new()
        .num_threads(num_cpus * 2)
        .build_global()
        .unwrap();
}

// Efficient parallel tensor operations
fn parallel_tensor_operation(input: &Tensor) -> Result<Tensor> {
    let data = input.data()?;
    let chunk_size = data.len() / rayon::current_num_threads();
    
    let result: Vec<f32> = data
        .par_chunks(chunk_size)
        .flat_map(|chunk| {
            chunk.iter().map(|&x| expensive_operation(x))
        })
        .collect();
    
    Tensor::from_slice(&result, input.shape().dims())
}
```

## Model Architecture Optimization

### 1. **Layer Fusion**

#### Automatic Fusion
```rust
use torsh_fx::passes::FusionPass;

// Fuse consecutive operations
let fusion_pass = FusionPass::new()
    .enable_conv_bn_fusion()
    .enable_linear_activation_fusion()
    .enable_elementwise_fusion();

let optimized_model = fusion_pass.apply(model)?;

// Manual fusion for custom patterns
fn fused_conv_bn_relu(
    input: &Tensor,
    weight: &Tensor,
    bias: &Tensor,
    bn_weight: &Tensor,
    bn_bias: &Tensor,
    bn_mean: &Tensor,
    bn_var: &Tensor,
    eps: f32
) -> Result<Tensor> {
    // Fuse conv + batch norm + ReLU into single operation
    torsh_backend::cuda::fused_conv_bn_relu(
        input, weight, bias, bn_weight, bn_bias, bn_mean, bn_var, eps
    )
}
```

#### Graph Optimization
```rust
use torsh_fx::graph::GraphOptimizer;

let optimizer = GraphOptimizer::new()
    .enable_constant_folding()
    .enable_dead_code_elimination()
    .enable_common_subexpression_elimination()
    .enable_operator_fusion();

let optimized_graph = optimizer.optimize(model.graph())?;
```

### 2. **Pruning and Sparsity**

#### Structured Pruning
```rust
use torsh_nn::pruning::{StructuredPruner, PruningStrategy};

// Remove entire channels/filters
let pruner = StructuredPruner::new(PruningStrategy::ChannelWise)
    .with_sparsity_target(0.5) // Remove 50% of channels
    .with_importance_metric(ImportanceMetric::L1Norm);

let pruned_model = pruner.prune(&model, &calibration_data)?;

// Measure speedup
let original_latency = benchmark_inference(&model, &test_data)?;
let pruned_latency = benchmark_inference(&pruned_model, &test_data)?;
println!("Pruning speedup: {:.2}x", 
    original_latency.as_secs_f64() / pruned_latency.as_secs_f64());
```

#### Unstructured Pruning
```rust
use torsh_nn::pruning::UnstructuredPruner;

// Remove individual weights
let pruner = UnstructuredPruner::new()
    .with_sparsity_target(0.9) // 90% sparsity
    .with_magnitude_based_pruning();

let sparse_model = pruner.prune(&model)?;

// Use sparse kernels for inference
let sparse_output = sparse_model.forward_sparse(&input)?;
```

### 3. **Knowledge Distillation**

#### Teacher-Student Training
```rust
use torsh_nn::distillation::KnowledgeDistillation;

// Distill large model to smaller one
let distillation = KnowledgeDistillation::new()
    .with_teacher_model(large_model)
    .with_student_model(small_model)
    .with_temperature(4.0)
    .with_alpha(0.7); // Weight for distillation loss

for (inputs, targets) in dataloader {
    // Get teacher predictions
    let teacher_logits = teacher_model.forward(&inputs)?;
    
    // Train student with distillation
    let student_logits = student_model.forward(&inputs)?;
    let distillation_loss = distillation.compute_loss(
        &student_logits, &teacher_logits, &targets
    )?;
    
    distillation_loss.backward()?;
    optimizer.step();
}
```

## Advanced Optimization Techniques

### 1. **Dynamic Shape Optimization**

#### Shape Inference and Caching
```rust
use torsh_fx::dynamic_shapes::ShapeInference;

struct CachedShapeInference {
    cache: HashMap<Vec<usize>, Vec<usize>>,
    inference_engine: ShapeInference,
}

impl CachedShapeInference {
    fn infer_output_shape(&mut self, input_shape: &[usize]) -> Vec<usize> {
        if let Some(cached) = self.cache.get(input_shape) {
            return cached.clone();
        }
        
        let output_shape = self.inference_engine.infer(input_shape);
        self.cache.insert(input_shape.to_vec(), output_shape.clone());
        output_shape
    }
}
```

#### Operator Specialization
```rust
// Specialize operations for common shapes
fn specialized_conv2d(input: &Tensor, weight: &Tensor) -> Result<Tensor> {
    match (input.shape().dims(), weight.shape().dims()) {
        // Common ResNet block: 224x224 -> 112x112
        ([_, 64, 224, 224], [128, 64, 3, 3]) => {
            optimized_conv2d_224_to_112(input, weight)
        }
        // Common bottleneck: 56x56 -> 56x56
        ([_, 256, 56, 56], [128, 256, 1, 1]) => {
            optimized_conv2d_1x1_same_size(input, weight)
        }
        // Fallback to general implementation
        _ => general_conv2d(input, weight)
    }
}
```

### 2. **Compilation and JIT Optimization**

#### Graph Compilation
```rust
use torsh_jit::{GraphCompiler, CompilationOptions};

// Compile model for target hardware
let compiler = GraphCompiler::new()
    .with_target_device(Device::cuda())
    .with_optimization_level(OptimizationLevel::Aggressive)
    .enable_kernel_fusion()
    .enable_memory_optimization();

let compiled_model = compiler.compile(&model)?;

// JIT compilation for dynamic shapes
let jit_model = compiler.compile_with_dynamic_shapes(
    &model,
    &input_shape_ranges
)?;
```

#### Custom Kernel Generation
```rust
use torsh_jit::kernel_generator::KernelGenerator;

// Generate optimized kernels for specific operations
let kernel_gen = KernelGenerator::new()
    .with_template("elementwise_binary")
    .with_operation("x + y * 2.0")
    .with_vectorization(true);

let custom_kernel = kernel_gen.generate_for_shapes(&input_shapes)?;
let result = custom_kernel.execute(&[&input1, &input2])?;
```

### 3. **Memory Optimization**

#### Gradient Accumulation
```rust
// Simulate larger batch sizes with gradient accumulation
let effective_batch_size = 128;
let micro_batch_size = 16;
let accumulation_steps = effective_batch_size / micro_batch_size;

let mut accumulated_loss = 0.0;

for step in 0..accumulation_steps {
    let micro_batch = get_micro_batch(step, micro_batch_size);
    let output = model.forward(&micro_batch.inputs)?;
    let loss = criterion(&output, &micro_batch.targets)?;
    
    // Scale loss by accumulation steps
    let scaled_loss = loss / accumulation_steps as f32;
    scaled_loss.backward()?;
    
    accumulated_loss += loss.item();
}

// Update parameters after accumulation
optimizer.step();
optimizer.zero_grad();
```

#### Memory-Mapped Tensors
```rust
use torsh_tensor::storage::MemoryMappedStorage;

// Use memory-mapped storage for large models
let storage = MemoryMappedStorage::new("large_model_weights.bin")?
    .with_lazy_loading(true)
    .with_cache_size(1_000_000_000); // 1GB cache

let tensor = Tensor::from_storage(storage, shape, dtype)?;
```

## Performance Monitoring and Profiling

### 1. **Real-time Performance Monitoring**

#### Performance Metrics Collection
```rust
use torsh_profiler::{PerformanceMonitor, MetricType};

let monitor = PerformanceMonitor::new()
    .track_metric(MetricType::Latency)
    .track_metric(MetricType::Throughput)
    .track_metric(MetricType::MemoryUsage)
    .track_metric(MetricType::GpuUtilization);

// Monitor training loop
for epoch in 0..num_epochs {
    monitor.start_epoch();
    
    for batch in dataloader {
        monitor.start_batch();
        
        let output = model.forward(&batch.inputs)?;
        let loss = criterion(&output, &batch.targets)?;
        loss.backward()?;
        optimizer.step();
        
        monitor.end_batch();
        
        // Log metrics periodically
        if monitor.should_log() {
            let metrics = monitor.get_current_metrics();
            println!("Throughput: {:.1} samples/sec", metrics.throughput);
            println!("Memory: {:.1} GB", metrics.memory_usage_gb);
        }
    }
    
    monitor.end_epoch();
}
```

### 2. **Bottleneck Detection**

#### Automated Performance Analysis
```rust
use torsh_profiler::BottleneckDetector;

let detector = BottleneckDetector::new()
    .with_threshold(0.1) // 10% of total time
    .enable_layer_profiling()
    .enable_memory_profiling();

let profile = detector.profile_model(&model, &sample_inputs)?;

for bottleneck in profile.bottlenecks() {
    match bottleneck.category {
        BottleneckCategory::Compute => {
            println!("Compute bottleneck in {}: {:.2}ms", 
                bottleneck.layer_name, bottleneck.time_ms);
        }
        BottleneckCategory::Memory => {
            println!("Memory bottleneck: {:.1} GB/s bandwidth", 
                bottleneck.memory_bandwidth_gbs);
        }
        BottleneckCategory::Synchronization => {
            println!("GPU synchronization overhead: {:.2}ms", 
                bottleneck.sync_overhead_ms);
        }
    }
}
```

## Platform-Specific Optimizations

### 1. **Mobile/Edge Optimization**

#### Model Quantization for Mobile
```rust
use torsh_quantization::mobile::MobileOptimizer;

let mobile_optimizer = MobileOptimizer::new()
    .target_platform(Platform::AndroidARM64)
    .target_memory_mb(512)
    .target_latency_ms(50)
    .prefer_accuracy(0.95);

let mobile_model = mobile_optimizer.optimize(&model)?;

// Benchmark on target hardware
let mobile_benchmark = MobileBenchmark::new()
    .with_thermal_throttling_detection()
    .with_battery_usage_monitoring();

let results = mobile_benchmark.run(&mobile_model)?;
```

### 2. **Cloud/Server Optimization**

#### Multi-GPU Deployment
```rust
use torsh_distributed::DataParallel;

// Distribute model across multiple GPUs
let devices = vec![Device::cuda(0), Device::cuda(1), Device::cuda(2), Device::cuda(3)];
let parallel_model = DataParallel::new(model, devices)?;

// Optimize for throughput
for batch in large_dataloader {
    let distributed_batch = parallel_model.distribute_batch(&batch)?;
    let outputs = parallel_model.forward(&distributed_batch)?;
    let loss = parallel_model.compute_loss(&outputs, &batch.targets)?;
    
    loss.backward()?;
    parallel_model.synchronize_gradients()?;
    optimizer.step();
}
```

## Debugging Performance Issues

### 1. **Performance Regression Testing**

#### Automated Performance Testing
```rust
use torsh_benches::regression::RegressionTester;

let tester = RegressionTester::new()
    .with_baseline_commit("main")
    .with_significance_threshold(0.05)
    .with_performance_threshold(0.02); // 2% regression threshold

// Test for regressions
let regression_report = tester.test_current_commit()?;

if regression_report.has_regressions() {
    for regression in regression_report.regressions() {
        println!("Regression in {}: {:.1}% slower", 
            regression.benchmark_name, 
            regression.slowdown_percent);
    }
}
```

### 2. **Memory Leak Detection**

#### Memory Usage Tracking
```rust
use torsh_profiler::MemoryTracker;

let tracker = MemoryTracker::new()
    .track_allocations(true)
    .track_deallocations(true)
    .detect_leaks(true);

tracker.start();

// Run your training loop
for epoch in 0..num_epochs {
    for batch in dataloader {
        let output = model.forward(&batch.inputs)?;
        let loss = criterion(&output, &batch.targets)?;
        loss.backward()?;
        optimizer.step();
        optimizer.zero_grad();
        
        // Check for memory growth
        if tracker.memory_growth_exceeds_threshold() {
            let leak_report = tracker.generate_leak_report();
            println!("Potential memory leak detected: {}", leak_report);
            break;
        }
    }
}

tracker.stop();
```

## Best Practices Summary

### 1. **General Guidelines**

- **Profile first**: Always measure before optimizing
- **Focus on hotspots**: Optimize the 20% of code that takes 80% of time
- **Validate improvements**: Measure performance gains and accuracy retention
- **Consider trade-offs**: Balance speed vs. memory vs. accuracy

### 2. **Development Workflow**

1. **Establish baseline**: Measure current performance
2. **Identify bottlenecks**: Use profiling tools to find slow operations
3. **Apply optimizations**: Start with high-impact, low-effort changes
4. **Validate results**: Ensure optimizations provide expected benefits
5. **Monitor continuously**: Set up regression testing

### 3. **Hardware-Specific Tips**

#### GPU Optimization Checklist
- [ ] Use appropriate batch sizes (powers of 2)
- [ ] Enable mixed precision training
- [ ] Utilize Tensor Cores where possible
- [ ] Minimize CPU-GPU data transfers
- [ ] Use asynchronous operations
- [ ] Optimize memory access patterns

#### CPU Optimization Checklist
- [ ] Enable SIMD vectorization
- [ ] Optimize cache usage patterns
- [ ] Use appropriate thread counts
- [ ] Minimize memory allocations
- [ ] Consider NUMA topology
- [ ] Use cache-friendly data layouts

This optimization guide provides a comprehensive foundation for maximizing ToRSh performance across different deployment scenarios and hardware configurations.